{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will read file \"/home/tshore/Projects/tangrams-restricted/Data/wordscores.tsv\".\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "import sys\n",
    "\n",
    "def get_project_root() -> str:\n",
    "    system = platform.system()\n",
    "    return r\"D:\\Users\\tcshore\\Documents\\Projects\\Tangrams\\Data\" if system == \"Windows\" else \"/home/tshore/Projects/tangrams-restricted/Data\"\n",
    "\n",
    "infile_path = os.path.join(get_project_root(), \"wordscores.tsv\")\n",
    "print(\"Will read file \\\"{}\\\".\".format(infile_path), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading \"/home/tshore/Projects/tangrams-restricted/Data/wordscores.tsv\" using encoding \"windows-1252\".\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "RESULTS_FILE_CSV_DIALECT = csv.excel_tab\n",
    "# NOTE: \"category\" dtype doesn't work with pandas-0.21.0 but does with pandas-0.21.1\n",
    "__RESULTS_FILE_DTYPES = {\"DYAD\": \"category\", \"WORD\": \"category\", \"IS_TARGET\": bool, \"IS_OOV\": bool,\n",
    "\t\t\t\t \"IS_INSTRUCTOR\": bool, \"SHAPE\": \"category\", \"ONLY_INSTRUCTOR\": bool, \"WEIGHT_BY_FREQ\": bool}\n",
    "\n",
    "def read_results_file(inpath: str, encoding: str) -> pd.DataFrame:\n",
    "\tprint(\"Reading \\\"{}\\\" using encoding \\\"{}\\\".\".format(inpath, encoding), file=sys.stderr)\n",
    "\tresult = pd.read_csv(inpath, dialect=RESULTS_FILE_CSV_DIALECT, sep=RESULTS_FILE_CSV_DIALECT.delimiter,\n",
    "\t\t\t\t\t\t float_precision=\"round_trip\",\n",
    "\t\t\t\t\t\t encoding=encoding, memory_map=True, dtype=__RESULTS_FILE_DTYPES)\n",
    "\treturn result\n",
    "\n",
    "cv_results = read_results_file(infile_path, \"windows-1252\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 1206840 cross-validation results for 39 dyad(s).\n",
      "Found 20 unique entity IDs.\n"
     ]
    }
   ],
   "source": [
    "print(\"Read {} cross-validation results for {} dyad(s).\".format(cv_results.shape[0], len(cv_results[\"DYAD\"].unique())),\n",
    "      file=sys.stderr)\n",
    "entity_ids = frozenset(cv_results[\"ENTITY\"].unique())\n",
    "print(\"Found {} unique entity IDs.\".format(len(entity_ids)), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found 60342 utterance tokens for all cross-validations.\n"
     ]
    }
   ],
   "source": [
    "def are_all_entities_represented(df: pd.DataFrame, entity_ids) -> bool:\n",
    "\t\"\"\"\n",
    "\tChecks if all entities are represented for each individual token in each utterance in the dataframe.\n",
    "    \n",
    "\t:param df: The dataframe to check.\n",
    "\t:param entity_ids: A collection of all unique entity IDs.\n",
    "\t:return: true iff for each token in each utterance there is one row for each entity ID.\n",
    "\t\"\"\"\n",
    "\tutt_toks = df.groupby(\n",
    "\t\t(\"CROSS_VALIDATION_ITER\", \"DYAD\", \"ROUND\", \"UTT_START_TIME\", \"UTT_END_TIME\", \"WORD\", \"TOKEN_SEQ_ORDINALITY\"), as_index=False)\n",
    "\tprint(\"Found {} utterance tokens for all cross-validations.\".format(len(utt_toks)), file=sys.stderr)\n",
    "\t# Check if there is a row for each entity (possible referent) for each token\n",
    "\treturn all(utt_toks.apply(lambda group: is_collection_equivalent(group[\"ENTITY\"], entity_ids)))\n",
    "\n",
    "def is_collection_equivalent(c1, c2) -> bool:\n",
    "\tc1_len = len(c1)\n",
    "\tc2_len = len(c2)\n",
    "\treturn c1_len == c2_len and all(elem in c2 for elem in c1)\n",
    "\n",
    "assert are_all_entities_represented(cv_results, entity_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found 60342 nontarget rows and 1146498 target rows. Ratio: 19.0\n"
     ]
    }
   ],
   "source": [
    "def find_target_ref_rows(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    result = df.loc[df[\"IS_TARGET\"] == True]\n",
    "    result_row_count = result.shape[0]\n",
    "    complement_row_count = df.loc[~df.index.isin(result.index)].shape[0]\n",
    "    assert result_row_count + complement_row_count == df.shape[0]\n",
    "    print(\"Found {} nontarget rows and {} target rows. Ratio: {}\".format(result_row_count, complement_row_count, complement_row_count / float(result_row_count)), file=sys.stderr)\n",
    "    return result\n",
    "\n",
    "cv_results = find_target_ref_rows(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting token sequences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:0\n",
      "     CROSS_VALIDATION_ITER                           DYAD  ROUND  \\\n",
      "178                      1  20170328-1224-patrik-testRavi      1   \n",
      "179                      1  20170328-1224-patrik-testRavi      1   \n",
      "180                      1  20170328-1224-patrik-testRavi      1   \n",
      "181                      1  20170328-1224-patrik-testRavi      1   \n",
      "182                      1  20170328-1224-patrik-testRavi      1   \n",
      "183                      1  20170328-1224-patrik-testRavi      1   \n",
      "184                      1  20170328-1224-patrik-testRavi      1   \n",
      "185                      1  20170328-1224-patrik-testRavi      1   \n",
      "\n",
      "     ROUND_START_TIME  GAME_SCORE  UTT_START_TIME  UTT_END_TIME  \\\n",
      "178            60.475           0         60.8083       63.0015   \n",
      "179            60.475           0         60.8083       63.0015   \n",
      "180            60.475           0         60.8083       63.0015   \n",
      "181            60.475           0         60.8083       63.0015   \n",
      "182            60.475           0         60.8083       63.0015   \n",
      "183            60.475           0         60.8083       63.0015   \n",
      "184            60.475           0         60.8083       63.0015   \n",
      "185            60.475           0         60.8083       63.0015   \n",
      "\n",
      "     TOKEN_SEQ_ORDINALITY  IS_INSTRUCTOR      WORD       ...        \\\n",
      "178                     1           True        ah       ...         \n",
      "179                     2           True      okay       ...         \n",
      "180                     3           True       now       ...         \n",
      "181                     4           True         I       ...         \n",
      "182                     5           True       see       ...         \n",
      "183                     6           True         a       ...         \n",
      "184                     7           True  selected       ...         \n",
      "185                     8           True     piece       ...         \n",
      "\n",
      "     POSITION_X  POSITION_Y  MID_X  MID_Y  DISCOUNT ONLY_INSTRUCTOR  \\\n",
      "178        0.75         0.3    0.5    0.6         3           False   \n",
      "179        0.75         0.3    0.5    0.6         3           False   \n",
      "180        0.75         0.3    0.5    0.6         3           False   \n",
      "181        0.75         0.3    0.5    0.6         3           False   \n",
      "182        0.75         0.3    0.5    0.6         3           False   \n",
      "183        0.75         0.3    0.5    0.6         3           False   \n",
      "184        0.75         0.3    0.5    0.6         3           False   \n",
      "185        0.75         0.3    0.5    0.6         3           False   \n",
      "\n",
      "     RANDOM_SEED  TRAINING_SET_SIZE_DISCOUNT  UPDATE_WEIGHT  WEIGHT_BY_FREQ  \n",
      "178            1                           0              0           False  \n",
      "179            1                           0              0           False  \n",
      "180            1                           0              0           False  \n",
      "181            1                           0              0           False  \n",
      "182            1                           0              0           False  \n",
      "183            1                           0              0           False  \n",
      "184            1                           0              0           False  \n",
      "185            1                           0              0           False  \n",
      "\n",
      "[8 rows x 32 columns]\n",
      "Iter:1\n",
      "     CROSS_VALIDATION_ITER                           DYAD  ROUND  \\\n",
      "178                      1  20170328-1224-patrik-testRavi      1   \n",
      "179                      1  20170328-1224-patrik-testRavi      1   \n",
      "180                      1  20170328-1224-patrik-testRavi      1   \n",
      "181                      1  20170328-1224-patrik-testRavi      1   \n",
      "182                      1  20170328-1224-patrik-testRavi      1   \n",
      "183                      1  20170328-1224-patrik-testRavi      1   \n",
      "184                      1  20170328-1224-patrik-testRavi      1   \n",
      "185                      1  20170328-1224-patrik-testRavi      1   \n",
      "\n",
      "     ROUND_START_TIME  GAME_SCORE  UTT_START_TIME  UTT_END_TIME  \\\n",
      "178            60.475           0         60.8083       63.0015   \n",
      "179            60.475           0         60.8083       63.0015   \n",
      "180            60.475           0         60.8083       63.0015   \n",
      "181            60.475           0         60.8083       63.0015   \n",
      "182            60.475           0         60.8083       63.0015   \n",
      "183            60.475           0         60.8083       63.0015   \n",
      "184            60.475           0         60.8083       63.0015   \n",
      "185            60.475           0         60.8083       63.0015   \n",
      "\n",
      "     TOKEN_SEQ_ORDINALITY  IS_INSTRUCTOR      WORD       ...        \\\n",
      "178                     1           True        ah       ...         \n",
      "179                     2           True      okay       ...         \n",
      "180                     3           True       now       ...         \n",
      "181                     4           True         I       ...         \n",
      "182                     5           True       see       ...         \n",
      "183                     6           True         a       ...         \n",
      "184                     7           True  selected       ...         \n",
      "185                     8           True     piece       ...         \n",
      "\n",
      "     POSITION_X  POSITION_Y  MID_X  MID_Y  DISCOUNT ONLY_INSTRUCTOR  \\\n",
      "178        0.75         0.3    0.5    0.6         3           False   \n",
      "179        0.75         0.3    0.5    0.6         3           False   \n",
      "180        0.75         0.3    0.5    0.6         3           False   \n",
      "181        0.75         0.3    0.5    0.6         3           False   \n",
      "182        0.75         0.3    0.5    0.6         3           False   \n",
      "183        0.75         0.3    0.5    0.6         3           False   \n",
      "184        0.75         0.3    0.5    0.6         3           False   \n",
      "185        0.75         0.3    0.5    0.6         3           False   \n",
      "\n",
      "     RANDOM_SEED  TRAINING_SET_SIZE_DISCOUNT  UPDATE_WEIGHT  WEIGHT_BY_FREQ  \n",
      "178            1                           0              0           False  \n",
      "179            1                           0              0           False  \n",
      "180            1                           0              0           False  \n",
      "181            1                           0              0           False  \n",
      "182            1                           0              0           False  \n",
      "183            1                           0              0           False  \n",
      "184            1                           0              0           False  \n",
      "185            1                           0              0           False  \n",
      "\n",
      "[8 rows x 32 columns]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'split_seq_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-9cf4371ce691>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Splitting token sequences.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0mtoken_seq_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenSequenceFactory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesired_seq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0mword_seqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_seqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_seq_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m print(\"Split data into {} token sequences with a maximum sequence length of {}.\".format(len(word_seqs),\n\u001b[1;32m     61\u001b[0m                                                                                         desired_seq_len),\n",
      "\u001b[0;32m<ipython-input-13-9cf4371ce691>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0mword_seqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mscore_seqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0msplit_seq_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__split_rows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mword_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_seq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplit_seq_scores\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                         \u001b[0mword_seqs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/groupby.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0moption_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mode.chained_assignment'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_apply_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_python_apply_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/groupby.py\u001b[0m in \u001b[0;36m_python_apply_general\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    718\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_python_apply_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         keys, values, mutated = self.grouper.apply(f, self._selected_obj,\n\u001b[0;32m--> 720\u001b[0;31m                                                    self.axis)\n\u001b[0m\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         return self._wrap_applied_output(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/groupby.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, data, axis)\u001b[0m\n\u001b[1;32m   1800\u001b[0m             \u001b[0;31m# group might be modified\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1801\u001b[0m             \u001b[0mgroup_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_axes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1802\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1803\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_indexed_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_axes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1804\u001b[0m                 \u001b[0mmutated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-9cf4371ce691>\u001b[0m in \u001b[0;36m__split_rows\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0;31m#split_seq_words = np.array_split(seq_words, partition_count)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0;31m#split_seq_scores = np.array_split(seq_scores, partition_count)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msplit_seq_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_seq_scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'split_seq_words' is not defined"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class TokenSequenceFactory(object):\n",
    "\n",
    "\tdef __init__(self, seq_len: int):\n",
    "\t\tself.seq_len = seq_len\n",
    "\t\tself.iter = 0\n",
    "\n",
    "\tdef __call__(self, df: pd.DataFrame) -> Tuple[List[np.array], List[np.array]]:\n",
    "\t\t\"\"\"\n",
    "\t\tCreates a sequence of sequences of tokens, each representing an utterance, each of which thus causes an \"interruption\" in the chain\n",
    "\t\tso that e.g. the first token of one utterance is not learned as dependent on the last token of the utterance preceding it.\n",
    "\t\t:param df: The DataFrame to process.\n",
    "\t\t:return: Paired lists of 2D numpy arrays, each representing a sequence of datapoints which represents an utterance and the corresponding scores to predict.\n",
    "\t\t\"\"\"\n",
    "        # https://stackoverflow.com/a/47815400/1391325\n",
    "\t\tdf.sort_values(\"TOKEN_SEQ_ORDINALITY\", inplace=True)\n",
    "\t\tsequences = df.groupby((\"CROSS_VALIDATION_ITER\", \"DYAD\", \"ROUND\", \"UTT_START_TIME\", \"UTT_END_TIME\", \"ENTITY\"),\n",
    "\t\t\t\t\t\t\t   as_index=False)\n",
    "        \n",
    "\t\tword_seqs = []\n",
    "\t\tscore_seqs = []\n",
    "\t\tsplit_seq_scores = sequences.apply(self.__split_rows)\n",
    "\t\tfor word_seq, score_seq in split_seq_scores:\n",
    "\t\t\tword_seqs.extend(word_seq)\n",
    "\t\t\tscore_seqs.extend(score_seq)\n",
    "\t\tassert all(len(seq) == self.__seq_len for seq in word_seqs) \n",
    "\t\treturn word_seqs, score_seqs\n",
    "\n",
    "\tdef __split_rows(self, df: pd.DataFrame):\n",
    "\t\t#print(df[\"IS_TARGET\"].unique())\n",
    "\t\tprint(\"Iter:\" + str(self.iter))\n",
    "\t\t#print(df[\"CROSS_VALIDATION_ITER\"].unique())\n",
    "\t\t#print(df[\"WORD\"])\n",
    "\t\tprint(df)           \n",
    "\t\tself.iter += 1\n",
    "\t\t#max_ordinality = df[\"TOKEN_SEQ_ORDINALITY\"].max()\n",
    "\t\t#print(max_ordinality)\n",
    "\t\t#for window_end_ordinality in range(1, row_count + 1):\n",
    "\t\t#\twindow_start_ordinality = window_end_ordinality - self.seq_len\n",
    "\t\t#\tpreceding_window = df.loc[(df[\"TOKEN_SEQ_ORDINALITY\"] >= window_start_ordinality) & (df[\"TOKEN_SEQ_ORDINALITY\"] <= window_end_ordinality)]\n",
    "\t\t#\tprint(preceding_window)\n",
    "        \n",
    "\t\t#seq_words = df[\"WORD\"].values\n",
    "\t\t#seq_scores = df[\"PROBABILITY\"].values\n",
    "        \n",
    "\t\t#partition_count = math.ceil(len(seq_words) / self.__seq_len_divisor)\n",
    "\t\t#split_seq_words = np.array_split(seq_words, partition_count)\n",
    "\t\t#split_seq_scores = np.array_split(seq_scores, partition_count)\n",
    "\t\treturn split_seq_words, split_seq_scores\n",
    "    \n",
    "    \n",
    "desired_seq_len = 4\n",
    "print(\"Splitting token sequences.\", file=sys.stderr)\n",
    "token_seq_factory = TokenSequenceFactory(desired_seq_len)\n",
    "word_seqs, score_seqs = token_seq_factory(cv_results)\n",
    "print(\"Split data into {} token sequences with a maximum sequence length of {}.\".format(len(word_seqs),\n",
    "                                                                                        desired_seq_len),\n",
    "      file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "def pad_sequence(word_score_seq: Tuple[np.array, np.array], min_length: int) -> Tuple[np.array, np.array]:\n",
    "\tword_seq, score_seq = word_score_seq\n",
    "\tword_count = len(word_seq)\n",
    "\tassert word_count == len(score_seq)\n",
    "\tlength_diff = min_length - word_count\n",
    "\tif length_diff > 0:\n",
    "\t\t# NOTE: creating an intermediate tuple is necessary\n",
    "\t\tpadding_words = np.full(length_diff, \"__PADDING__\")\n",
    "\t\tpadded_word_seq = np.concatenate((padding_words, word_seq), axis=0)\n",
    "\t\tassert len(padded_word_seq) == min_length\n",
    "\t\tpadding_scores = np.full(length_diff, 0.0)\n",
    "\t\tpadded_score_seq = np.concatenate((padding_scores, score_seq), axis=0)\n",
    "\t\tassert len(padded_score_seq) == min_length\n",
    "\t\tresult = padded_word_seq, padded_score_seq\n",
    "\telse:\n",
    "\t\tresult = word_seq, score_seq\n",
    "\n",
    "\treturn result\n",
    "\n",
    "def pad_sequences(word_score_seqs : Sequence[Tuple[np.array, np.array]], min_length: int):\n",
    "    for word_score_seq in word_score_seqs:\n",
    "        print(pad_sequence(word_score_seq, min_length))\n",
    "\n",
    "            \n",
    "pad_sequences(zip(word_seqs, score_seqs), desired_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for seq in score_seqs:\n",
    "    print(\"scoreseq\")\n",
    "    for s in seq:\n",
    "        print(\"s\")\n",
    "        for something in s:\n",
    "            print(something)\n",
    "\n",
    "\n",
    "for seq in word_seqs:\n",
    "    print(\"seq\")\n",
    "    for t in seq:\n",
    "        print(\"t\")\n",
    "        for something in t:\n",
    "            print(something)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "all_words = tuple(itertools.chain((\"__PADDING__\", ), cv_results[\"WORD\"].values))\n",
    "print(\"Converting {} vocabulary entries to integer labels.\".format(len(all_words)), file=sys.stderr)\n",
    "# integer encode <https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/>\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_words)\n",
    "for word_seq in word_seqs:\n",
    "    integer_label_seq = label_encoder.transform(word_seq)\n",
    "    print(integer_label_seq)\n",
    "#integer_label_seqs = tuple(label_encoder.transform(word_seq) for word_seq in word_seqs)\n",
    "#print(integer_label_seqs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
