{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will read file \"/home/tshore/Projects/tangrams-restricted/Data/wordscores.tsv\".\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "import sys\n",
    "\n",
    "def get_project_root() -> str:\n",
    "    system = platform.system()\n",
    "    return r\"D:\\Users\\tcshore\\Documents\\Projects\\Tangrams\\Data\" if system == \"Windows\" else \"/home/tshore/Projects/tangrams-restricted/Data\"\n",
    "\n",
    "infile_path = os.path.join(get_project_root(), \"wordscores.tsv\")\n",
    "print(\"Will read file \\\"{}\\\".\".format(infile_path), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading \"/home/tshore/Projects/tangrams-restricted/Data/wordscores.tsv\" using encoding \"windows-1252\".\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "RESULTS_FILE_CSV_DIALECT = csv.excel_tab\n",
    "# NOTE: \"category\" dtype doesn't work with pandas-0.21.0 but does with pandas-0.21.1\n",
    "__RESULTS_FILE_DTYPES = {\"DYAD\": \"category\", \"WORD\": \"category\", \"IS_TARGET\": bool, \"IS_OOV\": bool,\n",
    "\t\t\t\t \"IS_INSTRUCTOR\": bool, \"SHAPE\": \"category\", \"ONLY_INSTRUCTOR\": bool, \"WEIGHT_BY_FREQ\": bool}\n",
    "\n",
    "def read_results_file(inpath: str, encoding: str) -> pd.DataFrame:\n",
    "\tprint(\"Reading \\\"{}\\\" using encoding \\\"{}\\\".\".format(inpath, encoding), file=sys.stderr)\n",
    "\tresult = pd.read_csv(inpath, dialect=RESULTS_FILE_CSV_DIALECT, sep=RESULTS_FILE_CSV_DIALECT.delimiter,\n",
    "\t\t\t\t\t\t float_precision=\"round_trip\",\n",
    "\t\t\t\t\t\t encoding=encoding, memory_map=True, dtype=__RESULTS_FILE_DTYPES)\n",
    "\treturn result\n",
    "\n",
    "cv_results = read_results_file(infile_path, \"windows-1252\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 1206840 cross-validation results for 39 dyad(s).\n",
      "Found 20 unique entity IDs.\n"
     ]
    }
   ],
   "source": [
    "print(\"Read {} cross-validation results for {} dyad(s).\".format(cv_results.shape[0], len(cv_results[\"DYAD\"].unique())),\n",
    "      file=sys.stderr)\n",
    "entity_ids = frozenset(cv_results[\"ENTITY\"].unique())\n",
    "print(\"Found {} unique entity IDs.\".format(len(entity_ids)), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found 60342 utterance tokens for all cross-validations.\n"
     ]
    }
   ],
   "source": [
    "def are_all_entities_represented(df: pd.DataFrame, entity_ids) -> bool:\n",
    "\t\"\"\"\n",
    "\tChecks if all entities are represented for each individual token in each utterance in the dataframe.\n",
    "    \n",
    "\t:param df: The dataframe to check.\n",
    "\t:param entity_ids: A collection of all unique entity IDs.\n",
    "\t:return: true iff for each token in each utterance there is one row for each entity ID.\n",
    "\t\"\"\"\n",
    "\tutt_toks = df.groupby(\n",
    "\t\t(\"CROSS_VALIDATION_ITER\", \"DYAD\", \"ROUND\", \"UTT_START_TIME\", \"UTT_END_TIME\", \"WORD\", \"TOKEN_SEQ_ORDINALITY\"), as_index=False)\n",
    "\tprint(\"Found {} utterance tokens for all cross-validations.\".format(len(utt_toks)), file=sys.stderr)\n",
    "\t# Check if there is a row for each entity (possible referent) for each token\n",
    "\treturn all(utt_toks.apply(lambda group: is_collection_equivalent(group[\"ENTITY\"], entity_ids)))\n",
    "\n",
    "def is_collection_equivalent(c1, c2) -> bool:\n",
    "\tc1_len = len(c1)\n",
    "\tc2_len = len(c2)\n",
    "\treturn c1_len == c2_len and all(elem in c2 for elem in c1)\n",
    "\n",
    "assert are_all_entities_represented(cv_results, entity_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found 60342 nontarget rows and 1146498 target rows. Ratio: 19.0\n"
     ]
    }
   ],
   "source": [
    "def find_target_ref_rows(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    result = df.loc[df[\"IS_TARGET\"] == True]\n",
    "    result_row_count = result.shape[0]\n",
    "    complement_row_count = df.loc[~df.index.isin(result.index)].shape[0]\n",
    "    assert result_row_count + complement_row_count == df.shape[0]\n",
    "    print(\"Found {} nontarget rows and {} target rows. Ratio: {}\".format(result_row_count, complement_row_count, complement_row_count / float(result_row_count)), file=sys.stderr)\n",
    "    return result\n",
    "\n",
    "cv_results = find_target_ref_rows(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class TokenSequenceFactory(object):\n",
    "\n",
    "\tdef __init__(self, max_len: int):\n",
    "\t\tself.__max_len = max_len\n",
    "\t\tself.__max_len_divisor = float(max_len)\n",
    "\n",
    "\tdef __call__(self, df: pd.DataFrame) -> Tuple[List[np.array], List[np.array]]:\n",
    "\t\t\"\"\"\n",
    "\t\tCreates a sequence of sequences of tokens, each representing an utterance, each of which thus causes an \"interruption\" in the chain\n",
    "\t\tso that e.g. the first token of one utterance is not learned as dependent on the last token of the utterance preceding it.\n",
    "\t\t:param df: The DataFrame to process.\n",
    "\t\t:return: Paired lists of 2D numpy arrays, each representing a sequence of datapoints which represents an utterance and the corresponding scores to predict.\n",
    "\t\t\"\"\"\n",
    "        # https://stackoverflow.com/a/47815400/1391325\n",
    "\t\tdf.sort_values(\"TOKEN_SEQ_ORDINALITY\", inplace=True)\n",
    "\t\tsequences = df.groupby((\"CROSS_VALIDATION_ITER\", \"DYAD\", \"ROUND\", \"UTT_START_TIME\", \"UTT_END_TIME\", \"ENTITY\"),\n",
    "\t\t\t\t\t\t\t   as_index=False)\n",
    "\t\t#sequences.apply(lambda seq : self.create_word_forms(seq[\"ONEHOT_WORD_LABEL\"].values))\n",
    "        \n",
    "\t\tword_seqs = []\n",
    "\t\tscore_seqs = []\n",
    "\t\tmax_len_divisor = float(self.__max_len)\n",
    "\t\tsplit_seq_scores = sequences.apply(self.__split_row_values)\n",
    "\t\tfor word_seq, score_seq in split_seq_scores:\n",
    "\t\t\tword_seqs.extend(word_seq)\n",
    "\t\t\tscore_seqs.extend(score_seq)\n",
    "\t\tassert max(len(seq) for seq in word_seqs) <= self.__max_len\n",
    "\t\treturn word_seqs, score_seqs\n",
    "\n",
    "\tdef __split_row_values(self, df: pd.DataFrame) -> Tuple[np.array, np.array]:\n",
    "\t\tseq_words = df[\"WORD\"].values\n",
    "\t\tseq_scores = df[\"PROBABILITY\"].values\n",
    "        \n",
    "\t\tpartition_count = math.ceil(len(seq_words) / self.__max_len_divisor)\n",
    "\t\tsplit_seq_words = np.array_split(seq_words, partition_count)\n",
    "\t\tsplit_seq_scores = np.array_split(seq_scores, partition_count)\n",
    "\t\treturn split_seq_words, split_seq_scores\n",
    "\n",
    "desired_seq_len = 4\n",
    "print(\"Splitting token sequences.\", file=sys.stderr)\n",
    "token_seq_factory = TokenSequenceFactory(desired_seq_len)\n",
    "word_seqs, score_seqs = token_seq_factory(cv_results)\n",
    "print(\"Split data into {} token sequences with a maximum sequence length of {}.\".format(len(word_seqs),\n",
    "                                                                                        desired_seq_len),\n",
    "      file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "def pad_sequence(word_score_seq: Tuple[np.array, np.array], min_length: int) -> Tuple[np.array, np.array]:\n",
    "\tword_seq, score_seq = word_score_seq\n",
    "\tword_count = len(word_seq)\n",
    "\tassert word_count == len(score_seq)\n",
    "\tlength_diff = min_length - word_count\n",
    "\tif length_diff > 0:\n",
    "\t\t# NOTE: creating an intermediate tuple is necessary\n",
    "\t\tpadding_words = np.full(length_diff, \"__PADDING__\")\n",
    "\t\tpadded_word_seq = np.concatenate((padding_words, word_seq), axis=0)\n",
    "\t\tassert len(padded_word_seq) == min_length\n",
    "\t\tpadding_scores = np.full(length_diff, 0.0)\n",
    "\t\tpadded_score_seq = np.concatenate((padding_scores, score_seq), axis=0)\n",
    "\t\tassert len(padded_score_seq) == min_length\n",
    "\t\tresult = padded_word_seq, padded_score_seq\n",
    "\telse:\n",
    "\t\tresult = word_seq, score_seq\n",
    "\n",
    "\treturn result\n",
    "\n",
    "def pad_sequences(word_score_seqs : Sequence[Tuple[np.array, np.array]], min_length: int):\n",
    "    for word_score_seq in word_score_seqs:\n",
    "        print(pad_sequence(word_score_seq, min_length))\n",
    "\n",
    "            \n",
    "pad_sequences(zip(word_seqs, score_seqs), desired_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq in score_seqs:\n",
    "    print(\"scoreseq\")\n",
    "    for s in seq:\n",
    "        print(\"s\")\n",
    "        for something in s:\n",
    "            print(something)\n",
    "\n",
    "\n",
    "for seq in word_seqs:\n",
    "    print(\"seq\")\n",
    "    for t in seq:\n",
    "        print(\"t\")\n",
    "        for something in t:\n",
    "            print(something)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "all_words = tuple(itertools.chain((\"__PADDING__\", ), cv_results[\"WORD\"].values))\n",
    "print(\"Converting {} vocabulary entries to integer labels.\".format(len(all_words)), file=sys.stderr)\n",
    "# integer encode <https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/>\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_words)\n",
    "for word_seq in word_seqs:\n",
    "    integer_label_seq = label_encoder.transform(word_seq)\n",
    "    print(integer_label_seq)\n",
    "#integer_label_seqs = tuple(label_encoder.transform(word_seq) for word_seq in word_seqs)\n",
    "#print(integer_label_seqs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
