{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will read file \"D:\\Users\\tcshore\\Documents\\Projects\\Tangrams\\Data\\wordscores-inflected-small.tsv\".\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "import sys\n",
    "\n",
    "def get_project_root() -> str:\n",
    "    system = platform.system()\n",
    "    return r\"D:\\Users\\tcshore\\Documents\\Projects\\Tangrams\\Data\" if system == \"Windows\" else \"/home/tshore/Projects/tangrams-restricted/Data\"\n",
    "\n",
    "infile_path = os.path.join(get_project_root(), \"wordscores-inflected-small.tsv\")\n",
    "print(\"Will read file \\\"{}\\\".\".format(infile_path), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading \"D:\\Users\\tcshore\\Documents\\Projects\\Tangrams\\Data\\wordscores-inflected-small.tsv\" using encoding \"windows-1252\".\n",
      "Read 67060 cross-validation results for 3 dyad(s).\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "RESULTS_FILE_CSV_DIALECT = csv.excel_tab\n",
    "# NOTE: \"category\" dtype doesn't work with pandas-0.21.0 but does with pandas-0.21.1\n",
    "__RESULTS_FILE_DTYPES = {\"DYAD\": \"category\", \"ENTITY\" : \"category\", \"IS_TARGET\": bool, \"IS_OOV\": bool,\n",
    "\t\t\t\t \"IS_INSTRUCTOR\": bool, \"SHAPE\": \"category\", \"ONLY_INSTRUCTOR\": bool, \"WEIGHT_BY_FREQ\": bool}\n",
    "\n",
    "def read_results_file(inpath: str, encoding: str) -> pd.DataFrame:\n",
    "\tprint(\"Reading \\\"{}\\\" using encoding \\\"{}\\\".\".format(inpath, encoding), file=sys.stderr)\n",
    "\tresult = pd.read_csv(inpath, dialect=RESULTS_FILE_CSV_DIALECT, sep=RESULTS_FILE_CSV_DIALECT.delimiter,\n",
    "\t\t\t\t\t\t float_precision=\"round_trip\",\n",
    "\t\t\t\t\t\t encoding=encoding, memory_map=True, dtype=__RESULTS_FILE_DTYPES)\n",
    "\treturn result\n",
    "\n",
    "cv_results = read_results_file(infile_path, \"windows-1252\")\n",
    "print(\"Read {} cross-validation results for {} dyad(s).\".format(cv_results.shape[0], cv_results[\"DYAD\"].nunique()),\n",
    "      file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found 63707 nontarget rows and 3353 target rows. Ratio: 19.0\n"
     ]
    }
   ],
   "source": [
    "def find_target_ref_rows(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\tresult = df.loc[df[\"IS_TARGET\"] == True]\n",
    "\tresult_row_count = result.shape[0]\n",
    "\tcomplement_row_count = df.loc[~df.index.isin(result.index)].shape[0]\n",
    "\tassert result_row_count + complement_row_count == df.shape[0]\n",
    "\tprint(\"Found {} nontarget rows and {} target rows. Ratio: {}\".format(complement_row_count, result_row_count,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t complement_row_count / float(\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t result_row_count)), file=sys.stderr)\n",
    "\treturn result\n",
    "\n",
    "cv_results = find_target_ref_rows(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Replacing OOV words with label \"__OOV__\".\n"
     ]
    }
   ],
   "source": [
    "OOV_LABEL = \"__OOV__\"\n",
    "    \n",
    "print(\"Replacing OOV words with label \\\"{}\\\".\".format(OOV_LABEL), file=sys.stderr)\n",
    "cv_results.loc[cv_results[\"IS_OOV\"] == True, \"WORD\"] = OOV_LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting one-hot encoder for vocabulary of size 249.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#import numpy as np\n",
    "\n",
    "# Create vocab before splitting training and testing DFs so that the word feature set is stable\n",
    "print(\"Fitting one-hot encoder for vocabulary of size {}.\".format(cv_results[\"WORD\"].nunique()), file=sys.stderr)\n",
    "\n",
    "# https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "vocab_labels = label_encoder.fit_transform(cv_results[\"WORD\"])\n",
    "cv_results[\"WORD_LABEL\"] = vocab_labels\n",
    "#print(vocab_labels)\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "vocab_labels = vocab_labels.reshape(len(vocab_labels), 1)\n",
    "onehot_encoder.fit(vocab_labels)\n",
    "#assert onehot_encoder.n_values_ == len(vocab_words)\n",
    "#vocab_onehot_encoded = onehot_encoder.fit_transform(vocab_labels)\n",
    "#print(vocab_onehot_encoded)\n",
    "# invert first example\n",
    "#inverted = label_encoder.inverse_transform([np.argmax(vocab_onehot_encoded[0, :])])\n",
    "#print(inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training set dyads: ['20']\n",
      "Test set dyads: ['21', '22']\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def split_training_testing(df: pd.DataFrame, test_set_size : int) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "\tdyad_ids = df[\"DYAD\"].unique()\n",
    "\ttraining_set_size = len(dyad_ids) - test_set_size\n",
    "\tif training_set_size < 1:\n",
    "\t\traise ValueError(\"Desired test set size is {} but only {} dyads found.\".format(test_set_size, len(dyad_ids)))\n",
    "\telse:\n",
    "\t\ttraining_set_dyads = frozenset(np.random.choice(dyad_ids, training_set_size))\n",
    "\t\tprint(\"Training set dyads: {}\".format(sorted(training_set_dyads)), file=sys.stderr)\n",
    "\t\ttraining_set_idxs = df[\"DYAD\"].isin(training_set_dyads)\n",
    "\t\ttraining_set = df.loc[training_set_idxs]\n",
    "\t\ttest_set = df.loc[~training_set_idxs]\n",
    "\t\ttest_set_dyads = frozenset(test_set[\"DYAD\"].unique())\n",
    "\t\tprint(\"Test set dyads: {}\".format(sorted(test_set_dyads)), file=sys.stderr)\n",
    "\n",
    "\t\tassert not frozenset(training_set[\"DYAD\"].unique()).intersection(frozenset(test_set_dyads))\n",
    "\t\treturn training_set, test_set\n",
    "    \n",
    "# https://stackoverflow.com/a/47815400/1391325\n",
    "cv_results.sort_values(\"TOKEN_SEQ_ORDINALITY\", inplace=True)\n",
    "training_df, test_df = split_training_testing(cv_results, 1)\n",
    "#training_df = training_df.copy(deep=False)\n",
    "#test_df = test_df.copy(deep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting token sequences.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (11, 251), indices imply (11, 33)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_arrays\u001b[1;34m(arrays, names, axes)\u001b[0m\n\u001b[0;32m   4637\u001b[0m         \u001b[0mblocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mform_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4638\u001b[1;33m         \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBlockManager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4639\u001b[0m         \u001b[0mmgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, blocks, axes, do_integrity_check, fastpath)\u001b[0m\n\u001b[0;32m   3032\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdo_integrity_check\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3033\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_verify_integrity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3034\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36m_verify_integrity\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3243\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_verify_integrity\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mmgr_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3244\u001b[1;33m                 \u001b[0mconstruction_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtot_items\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3245\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mtot_items\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mconstruction_error\u001b[1;34m(tot_items, block_shape, axes, e)\u001b[0m\n\u001b[0;32m   4607\u001b[0m     raise ValueError(\"Shape of passed values is {0}, indices imply {1}\".format(\n\u001b[1;32m-> 4608\u001b[1;33m         passed, implied))\n\u001b[0m\u001b[0;32m   4609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Shape of passed values is (11, 251), indices imply (11, 33)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-0b98bbcb3cac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Splitting token sequences.\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[0mseq_matrix_generator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequenceMatrixGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0monehot_encoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m \u001b[0mtraining_seqs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq_matrix_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[0mmax_training_seq_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtraining_seqs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Created a training dataset with a size of {} and a max sequence length of {}.\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_seqs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_training_seq_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-0b98bbcb3cac>\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, df)\u001b[0m\n\u001b[0;32m     47\u001b[0m                         \u001b[1;33m(\u001b[0m\u001b[1;34m\"CROSS_VALIDATION_ITER\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"DYAD\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"UTT_START_TIME\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"UTT_END_TIME\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ENTITY\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \t\t\tas_index=False, sort=False)\n\u001b[1;32m---> 49\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0msequence_groups\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__create_seq_feature_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Splitting token sequences.\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    803\u001b[0m         \u001b[1;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    804\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0moption_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mode.chained_assignment'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 805\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_apply_general\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    806\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    807\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_python_apply_general\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby.py\u001b[0m in \u001b[0;36m_python_apply_general\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    807\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_python_apply_general\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    808\u001b[0m         keys, values, mutated = self.grouper.apply(f, self._selected_obj,\n\u001b[1;32m--> 809\u001b[1;33m                                                    self.axis)\n\u001b[0m\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    811\u001b[0m         return self._wrap_applied_output(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, f, data, axis)\u001b[0m\n\u001b[0;32m   1967\u001b[0m             \u001b[1;31m# group might be modified\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1968\u001b[0m             \u001b[0mgroup_axes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_axes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1969\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1970\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_is_indexed_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup_axes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1971\u001b[0m                 \u001b[0mmutated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-0b98bbcb3cac>\u001b[0m in \u001b[0;36m__create_seq_feature_matrix\u001b[1;34m(self, df)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m__create_seq_feature_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__create_datapoint_feature_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m                 \u001b[1;31m# noinspection PyProtectedMember\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m                 \u001b[1;31m#return np.matrix(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, axis, broadcast, raw, reduce, args, **kwds)\u001b[0m\n\u001b[0;32m   4875\u001b[0m                         \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4876\u001b[0m                         \u001b[0mreduce\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4877\u001b[1;33m                         ignore_failures=ignore_failures)\n\u001b[0m\u001b[0;32m   4878\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4879\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_broadcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_apply_standard\u001b[1;34m(self, func, axis, ignore_failures, reduce)\u001b[0m\n\u001b[0;32m   4988\u001b[0m                 \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4989\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4990\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4991\u001b[0m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres_index\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4992\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    328\u001b[0m                                  dtype=dtype, copy=copy)\n\u001b[0;32m    329\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_init_dict\u001b[1;34m(self, data, index, columns, dtype)\u001b[0m\n\u001b[0;32m    459\u001b[0m             \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 461\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_arrays_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    462\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_init_ndarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_arrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[0;32m   6171\u001b[0m     \u001b[0maxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_ensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6173\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcreate_block_manager_from_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_arrays\u001b[1;34m(arrays, names, axes)\u001b[0m\n\u001b[0;32m   4640\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmgr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4641\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4642\u001b[1;33m         \u001b[0mconstruction_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4643\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mconstruction_error\u001b[1;34m(tot_items, block_shape, axes, e)\u001b[0m\n\u001b[0;32m   4606\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Empty data passed with indices specified.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4607\u001b[0m     raise ValueError(\"Shape of passed values is {0}, indices imply {1}\".format(\n\u001b[1;32m-> 4608\u001b[1;33m         passed, implied))\n\u001b[0m\u001b[0;32m   4609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4610\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Shape of passed values is (11, 251), indices imply (11, 33)"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from typing import Iterator, List, Mapping\n",
    "\n",
    "import keras.preprocessing.sequence\n",
    "\n",
    "class SequenceMatrixGenerator(object):\n",
    "\n",
    "\tdef __init__(self, onehot_encoder):\n",
    "\t\tself.onehot_encoder = onehot_encoder\n",
    "\n",
    "\t@property\n",
    "\tdef feature_count(self) -> int:\n",
    "\t\tword_features = self.onehot_encoder.n_values_[0]\n",
    "\t\treturn word_features + 2\n",
    "\n",
    "\tdef __create_datapoint_feature_array(self, row: pd.Series) -> np.array:\n",
    "\t\t# word_features = [0.0] * len(self.__vocab_idxs)\n",
    "\t\t# The features representing each individual vocabulary word are at the beginning of the feature vector\n",
    "\t\t# word_features[self.__vocab_idxs[row[\"WORD\"]]] = 1.0\n",
    "\t\t# word_label = self.label_encoder.transform(row[\"WORD\"])\n",
    "\t\tword_label = row[\"WORD_LABEL\"]\n",
    "\t\t# print(\"Word label: {}\".format(word_label), file=sys.stderr)\n",
    "\t\t# \"OneHotEncoder.transform(..)\" returns a matrix even if only a single value is passed to it, so get just the first (and only) row\n",
    "\t\tword_features = self.onehot_encoder.transform(word_label)[0]\n",
    "\t\t# print(\"Word features: {}\".format(word_features), file=sys.stderr)\n",
    "\t\t# The word label for the one-hot encoding is that with the same index as the column that has a \"1\" value, i.e. the highest value in the vector of one-hot encoding values\n",
    "\t\t# inverse_label = np.argmax(word_features)\n",
    "\t\t# assert inverse_label == word_label\n",
    "\t\t# inverse_word = self.label_encoder.inverse_transform([inverse_label])\n",
    "\t\t# print(\"Inverse word label: {}\".format(inverse_label), file=sys.stderr)\n",
    "\t\tis_instructor = 1.0 if row[\"IS_INSTRUCTOR\"] else 0.0\n",
    "\t\t# is_target = 1.0 if row[\"IS_TARGET\"] else 0.0\n",
    "\t\tscore = row[\"PROBABILITY\"]\n",
    "\t\tother_features = np.array((is_instructor, score))\n",
    "\t\t# result = word_features + other_features\n",
    "\t\tresult = np.concatenate((word_features, other_features))\n",
    "\t\t# print(\"Created a vector of {} features.\".format(len(result)), file=sys.stderr)\n",
    "\t\treturn result\n",
    "\n",
    "\tdef __create_seq_feature_matrix(self, df: pd.DataFrame) -> np.matrix:\n",
    "\t\t# noinspection PyProtectedMember\n",
    "\t\treturn np.matrix(\n",
    "\t\t\ttuple(self.__create_datapoint_feature_array(row._asdict()) for row in df.itertuples(index=False)))\n",
    "\n",
    "\tdef __call__(self, df: pd.DataFrame) -> Iterator[np.matrix]:\n",
    "\t\tsequence_groups = df.groupby(\n",
    "\t\t\t(\"CROSS_VALIDATION_ITER\", \"DYAD\", \"UTT_START_TIME\", \"UTT_END_TIME\", \"ENTITY\"),\n",
    "\t\t\tas_index=False, sort=False)\n",
    "\t\treturn sequence_groups.apply(self.__create_seq_feature_matrix)\n",
    "\n",
    "print(\"Splitting token sequences.\", file=sys.stderr)\n",
    "seq_matrix_generator = SequenceMatrixGenerator(onehot_encoder)\n",
    "training_seqs = tuple(seq_matrix_generator(training_df))\n",
    "max_training_seq_len = max(m.shape[0] for m in training_seqs)\n",
    "print(\"Created a training dataset with a size of {} and a max sequence length of {}.\".format(len(training_seqs), max_training_seq_len), file=sys.stderr)\n",
    "test_seqs = tuple(seq_matrix_generator(test_df))\n",
    "max_test_seq_len = max(m.shape[0] for m in test_seqs)\n",
    "print(\"Created a test dataset with a size of {} and a max sequence length of {}.\".format(len(test_seqs), max_test_seq_len), file=sys.stderr)\n",
    "\n",
    "maxlen = max(max_training_seq_len, max_test_seq_len)\n",
    "print(\"Padding sequences to a length of {}.\".format(maxlen), file=sys.stderr)\n",
    "training_matrix = keras.preprocessing.sequence.pad_sequences(training_seqs, maxlen=maxlen, padding='pre', truncating='pre', value=0.)\n",
    "print(\"Batch training matrix shape: {}\".format(training_matrix.shape), file=sys.stderr)\n",
    "test_matrix = keras.preprocessing.sequence.pad_sequences(test_seqs, maxlen=maxlen, padding='pre', truncating='pre', value=0.)\n",
    "print(\"Batch test matrix shape: {}\".format(test_matrix.shape), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training X shape: (148, 40, 250)\n",
      "Training Y shape: (148, 40)\n",
      "Input shape: (40, 250)\n",
      "Units: 40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 40)                46560     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 40)                1640      \n",
      "=================================================================\n",
      "Total params: 48,200\n",
      "Trainable params: 48,200\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Sequential\n",
    "\n",
    "training_x = training_matrix[:, :, :-1]\n",
    "print(type(training_matrix))\n",
    "print(\"Training X shape: {}\".format(training_x.shape), file=sys.stderr)\n",
    "assert len(training_x.shape) == 3\n",
    "training_y = training_matrix[:, :, -1]\n",
    "print(\"Training Y shape: {}\".format(training_y.shape), file=sys.stderr)\n",
    "assert len(training_y.shape) == 2\n",
    "\n",
    "def create_model(training_x: np.ndarray, training_y: np.ndarray) -> Sequential:\n",
    "\tresult = Sequential()\n",
    "\t# word_embeddings = Embedding(len(vocab), embedding_vector_length, input_length=max_review_length)\n",
    "\t# model.add(word_embeddings)\n",
    "\t# model.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))\n",
    "\t# input shape is a pair of (timesteps, features) <https://stackoverflow.com/a/44583784/1391325>\n",
    "\tinput_shape = training_x.shape[1:]\n",
    "\tprint(\"Input shape: {}\".format(input_shape), file=sys.stderr)\n",
    "\tunits = training_y.shape[1]\n",
    "\tprint(\"Units: {}\".format(units), file=sys.stderr)\n",
    "\tlstm = LSTM(input_shape=input_shape, units=units)\n",
    "\t# lstm = LSTM(batch_input_shape = training_x.shape, stateful = True, units=len(training_y.shape))\n",
    "\tresult.add(lstm)\n",
    "\tresult.add(Dense(units, activation='sigmoid'))\n",
    "\tresult.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\tprint(result.summary())\n",
    "\treturn result\n",
    "\n",
    "model = create_model(training_x, training_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train LSTM\n",
    "\n",
    "#import keras.utils\n",
    "#import math\n",
    "\n",
    "#class CIFAR10Sequence(keras.utils.Sequence):\n",
    "\n",
    "#    def __init__(self, x_set, y_set, batch_size):\n",
    "#        self.x, self.y = x_set, y_set\n",
    "#        self.batch_size = batch_size\n",
    "\n",
    "#    def __len__(self):\n",
    "#        return math.ceil(len(self.x) / self.batch_size)\n",
    "\n",
    "#    def __getitem__(self, idx):\n",
    "#        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "#        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "#        return np.array(batch_x), np.array(batch_y)\n",
    "    \n",
    "#training_data_generator = CIFAR10Sequence(training_x, training_y, 32)\n",
    "\n",
    "#model.fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None)\n",
    "training_history = model.fit(training_x, training_y, epochs=1, verbose=1)\n",
    "#for seq_idx in range(0, len(training_x)):\n",
    "#   seq_x = training_x[seq_idx]\n",
    "#    print(seq_x.shape)\n",
    "#    seq_y = training_y[seq_idx]\n",
    "#    print(seq_y.shape)\n",
    "#    print(training_matrix.shape)\n",
    "#    training_x = np.array([training_matrix[:, :-1]])\n",
    "#print(training_x.shape)\n",
    "#    assert len(training_x.shape) == 3\n",
    "#    training_y = np.array([training_matrix[:, -1]])\n",
    "#print(training_y.shape)\n",
    "#    assert len(training_y.shape) == 3\n",
    "#    training_history = model.fit(x=training_x, y=training_y, epochs=1, verbose=1, batch_size=1)\n",
    "#    training_history = model.fit(x=seq_x, y=seq_y, epochs=1, verbose=1)\n",
    "#fit_generator(self, generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0)\n",
    "#training_history = model.fit_generator(training_data_generator, epochs=1, verbose=1)\n",
    "\n",
    "# train LSTM <https://machinelearningmastery.com/memory-in-a-long-short-term-memory-network/>\n",
    "#epochs = 5\n",
    "#for i in range(epochs):\n",
    "#    for seq_idx in range(0, training_x.shape[0]):\n",
    "        #x = training_x[i]\n",
    "#        x = training_x[0:31]\n",
    " #       x_oldshape = x.shape\n",
    " #       x_newshape = (-1, x_oldshape[0], x_oldshape[1])\n",
    "        #x = np.reshape(x, x_newshape)\n",
    "        #print(\"x.shape = {}\".format(x.shape), file=sys.stderr)\n",
    "        #y = training_y[i]\n",
    " #       y = training_y[0:31]\n",
    " #       y_oldshape = y.shape\n",
    " #       y_newshape = (-1, y_oldshape[0])\n",
    "        #y = np.reshape(y, y_newshape)\n",
    "        #print(\"y.shape = {}\".format(y.shape), file=sys.stderr)\n",
    " #       model.fit(x, y, epochs=1, batch_size=1, verbose=1, shuffle=False)\n",
    " #       model.reset_states()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test LSTM\n",
    "\n",
    "def onehot_encodings(features : np.array, onehot_encoder) -> np.array:\n",
    "    \"\"\"\n",
    "    :param features: A 2D array of feature vectors, each of which starts with one-hot encoding features. Any other features follow those.\n",
    "    :param onehot_encoder: The instance used to encode the original values.\n",
    "    :return: A 2D numpy array consisting only of one-hot encoding features.\n",
    "    \"\"\"\n",
    "    feature_count = onehot_encoder.n_values_[0]\n",
    "    return features[:,:feature_count + 1]\n",
    "\n",
    "\n",
    "test_x = test_matrix[:,:,:-1]\n",
    "print(test_x.shape)\n",
    "assert len(test_x.shape) == 3\n",
    "test_y = test_matrix[:,:,-1]\n",
    "print(test_y.shape)\n",
    "assert len(test_y.shape) == 2\n",
    "\n",
    "seq_predicted_values = model.predict(test_x, verbose=0)\n",
    "model.reset_states()\n",
    "#print(\"result.shape = {}\".format(seq_predicted_values.shape), file=sys.stderr)\n",
    "for i, tested_seq in enumerate(test_x):\n",
    "    word_features = onehot_encodings(tested_seq, onehot_encoder)\n",
    "    #print(\"word features: {}\".format(word_features))\n",
    "    #print(\"Tested sequence: {}\".format(tested_seq))\n",
    "    #print(\"Tested sequence shape: {}\".format(tested_seq.shape))\n",
    "    seq_labels = word_features.argmax(axis=1)\n",
    "    #print(\"Inverse labels: {}\".format(inverse_labels))\n",
    "    seq_words = label_encoder.inverse_transform(seq_labels)\n",
    "    print(\"Inverse word labels: {}\".format(seq_words))\n",
    "    predicted_values = seq_predicted_values[i]\n",
    "    assert tested_seq.shape[:-1] == predicted_values.shape\n",
    "    #assert tested_seq.shape[:-1] == actual_values.shape\n",
    "    #rint(\"Predicted values: {}\".format(predicted_values))\n",
    "    actual_values = test_y[i]\n",
    "    assert predicted_values.shape == actual_values.shape\n",
    "    \n",
    "    differences = predicted_values - actual_values\n",
    "    print(\"[Predicted_val - Actual_val]: {}\".format(differences))\n",
    "    #print(\"Actual values: {}\".format(actual_values))\n",
    "\t#print('X=%.1f y=%.1f, yhat=%.1f' % (seq1[i], seq1[i+1], result[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
