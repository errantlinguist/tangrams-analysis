{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will read file \"D:\\Users\\tcshore\\Documents\\Projects\\Tangrams\\Data\\wordprobseqs-inflected-small.tsv\".\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "import sys\n",
    "\n",
    "def get_project_root() -> str:\n",
    "    system = platform.system()\n",
    "    return r\"D:\\Users\\tcshore\\Documents\\Projects\\Tangrams\\Data\" if system == \"Windows\" else \"/home/tshore/Projects/tangrams-restricted/Data\"\n",
    "\n",
    "infile_path = os.path.join(get_project_root(), \"wordprobseqs-inflected-small.tsv\")\n",
    "print(\"Will read file \\\"{}\\\".\".format(infile_path), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading \"D:\\Users\\tcshore\\Documents\\Projects\\Tangrams\\Data\\wordprobseqs-inflected-small.tsv\" using encoding \"windows-1252\".\n",
      "Read 180880 cross-validation results for 2 dyad(s).\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "RESULTS_FILE_CSV_DIALECT = csv.excel_tab\n",
    "# NOTE: \"category\" dtype doesn't work with pandas-0.21.0 but does with pandas-0.21.1\n",
    "__RESULTS_FILE_DTYPES = {\"DYAD\": \"category\", \"WORD\": \"category\", \"IS_TARGET\": bool, \"IS_OOV\": bool,\n",
    "\t\t\t\t \"IS_INSTRUCTOR\": bool, \"SHAPE\": \"category\", \"ONLY_INSTRUCTOR\": bool, \"WEIGHT_BY_FREQ\": bool}\n",
    "\n",
    "def read_results_file(inpath: str, encoding: str) -> pd.DataFrame:\n",
    "\tprint(\"Reading \\\"{}\\\" using encoding \\\"{}\\\".\".format(inpath, encoding), file=sys.stderr)\n",
    "\tresult = pd.read_csv(inpath, dialect=RESULTS_FILE_CSV_DIALECT, sep=RESULTS_FILE_CSV_DIALECT.delimiter,\n",
    "\t\t\t\t\t\t float_precision=\"round_trip\",\n",
    "\t\t\t\t\t\t encoding=encoding, memory_map=True, dtype=__RESULTS_FILE_DTYPES)\n",
    "\treturn result\n",
    "\n",
    "cv_results = read_results_file(infile_path, \"windows-1252\")\n",
    "print(\"Read {} cross-validation results for {} dyad(s).\".format(cv_results.shape[0], cv_results[\"DYAD\"].nunique()),\n",
    "      file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found 171836 nontarget rows and 9044 target rows. Ratio: 19.0\n"
     ]
    }
   ],
   "source": [
    "def find_target_ref_rows(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\tresult = df.loc[df[\"IS_TARGET\"] == True]\n",
    "\tresult_row_count = result.shape[0]\n",
    "\tcomplement_row_count = df.loc[~df.index.isin(result.index)].shape[0]\n",
    "\tassert result_row_count + complement_row_count == df.shape[0]\n",
    "\tprint(\"Found {} nontarget rows and {} target rows. Ratio: {}\".format(complement_row_count, result_row_count,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t complement_row_count / float(\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t result_row_count)), file=sys.stderr)\n",
    "\treturn result\n",
    "\n",
    "cv_results = find_target_ref_rows(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting one-hot encoder for vocabulary of size 299.\n",
      "Creating vocab dictionary for one-hot label encoding.\n"
     ]
    }
   ],
   "source": [
    "# Create vocab before splitting training and testing DFs so that the word feature set is stable\n",
    "vocab = tuple(sorted(cv_results[\"WORD\"].unique()))\n",
    "print(\"Fitting one-hot encoder for vocabulary of size {}.\".format(len(vocab)), file=sys.stderr)\n",
    "# https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/\n",
    "print(\"Creating vocab dictionary for one-hot label encoding.\", file=sys.stderr)\n",
    "vocab_idxs = dict((word, idx) for (idx, word) in enumerate(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training set dyads: ['17']\n",
      "Test set dyads: ['1']\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def split_training_testing(df: pd.DataFrame, test_set_size : int) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "\tdyad_ids = df[\"DYAD\"].unique()\n",
    "\ttraining_set_size = len(dyad_ids) - test_set_size\n",
    "\tif training_set_size < 1:\n",
    "\t\traise ValueError(\"Desired test set size is {} but only {} dyads found.\".format(test_set_size, len(dyad_ids)))\n",
    "\telse:\n",
    "\t\ttraining_set_dyads = frozenset(np.random.choice(dyad_ids, training_set_size))\n",
    "\t\tprint(\"Training set dyads: {}\".format(sorted(training_set_dyads)), file=sys.stderr)\n",
    "\t\ttraining_set_idxs = df[\"DYAD\"].isin(training_set_dyads)\n",
    "\t\ttraining_set = df.loc[training_set_idxs]\n",
    "\t\ttest_set = df.loc[~training_set_idxs]\n",
    "\t\ttest_set_dyads = frozenset(test_set[\"DYAD\"].unique())\n",
    "\t\tprint(\"Test set dyads: {}\".format(sorted(test_set_dyads)), file=sys.stderr)\n",
    "\n",
    "\t\tassert not frozenset(training_set[\"DYAD\"].unique()).intersection(frozenset(test_set_dyads))\n",
    "\t\treturn training_set, test_set\n",
    "    \n",
    "training_df, test_df = split_training_testing(cv_results, 1)\n",
    "training_df = training_df.copy(deep=False)\n",
    "test_df = test_df.copy(deep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting token sequences.\n",
      "Created a training data matrix of shape (1174, 4, 302).\n",
      "Created a test data matrix of shape (1087, 4, 302).\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterator, List, Mapping\n",
    "\n",
    "class SequenceMatrixFactory(object):\n",
    "\n",
    "\tdef __init__(self,vocab_idxs : Mapping[str, int]):\n",
    "\t\tself.__vocab_idxs = vocab_idxs\n",
    "\t\tself.__feature_count = len(self.__vocab_idxs) + 3\n",
    "\n",
    "\tdef create_datapoint_feature_array(self, row: pd.Series) -> List[float]:\n",
    "\t\tword_features = [0.0] * len(self.__vocab_idxs)\n",
    "\t\t# The features representing each individual vocabulary word are at the beginning of the feature vector\n",
    "\t\tword_features[self.__vocab_idxs[row[\"WORD\"]]] = 1.0\n",
    "\t\tis_instructor = 1.0 if row[\"IS_INSTRUCTOR\"] else 0.0\n",
    "\t\tis_oov = 1.0 if row[\"IS_OOV\"] else 0.0\n",
    "\t\t#is_target = 1.0 if row[\"IS_TARGET\"] else 0.0\n",
    "\t\tscore = row[\"PROBABILITY\"]\n",
    "\t\tother_features = list((is_instructor, is_oov, score))\n",
    "\t\tresult = word_features + other_features\n",
    "\t\t#print(\"Created a vector of {} features.\".format(len(result)), file=sys.stderr)\n",
    "\t\treturn result\n",
    "\n",
    "\tdef __call__(self, df : pd.DataFrame) -> np.array:\n",
    "\t\t# https://stackoverflow.com/a/47815400/1391325\n",
    "\t\tdf.sort_values(\"TOKEN_SEQ_ORDINALITY\", inplace=True)\n",
    "\t\tsequence_groups = df.groupby((\"CROSS_VALIDATION_ITER\", \"DYAD\", \"SPLIT_SEQ_NO\", \"UTT_START_TIME\", \"UTT_END_TIME\", \"ENTITY\"), as_index=False)\n",
    "\t\treturn np.array(tuple(tuple(self.__create_feature_vectors(seq)) for _, seq in sequence_groups))\n",
    "        \n",
    "\tdef __create_feature_vectors(self, df : pd.DataFrame) -> Iterator[List[float]]:\n",
    "\t\t# noinspection PyProtectedMember\n",
    "\t\treturn (self.create_datapoint_feature_array(row._asdict()) for row in df.itertuples(index=False))\n",
    "\n",
    "print(\"Splitting token sequences.\", file=sys.stderr)\n",
    "seq_matrix_factory = SequenceMatrixFactory(vocab_idxs)\n",
    "training_matrix = seq_matrix_factory(training_df)\n",
    "print(\"Created a training data matrix of shape {}.\".format(training_matrix.shape), file=sys.stderr)\n",
    "test_matrix = seq_matrix_factory(test_df)\n",
    "print(\"Created a test data matrix of shape {}.\".format(test_matrix.shape), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1174, 4, 301)\n",
      "(1174, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input shape: (4, 301)\n",
      "Units: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 4)                 4896      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 20        \n",
      "=================================================================\n",
      "Total params: 4,916\n",
      "Trainable params: 4,916\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "training_x = training_matrix[:,:,:-1]\n",
    "print(training_x.shape)\n",
    "assert len(training_x.shape) == 3\n",
    "training_y = training_matrix[:,:,-1]\n",
    "print(training_y.shape)\n",
    "assert len(training_y.shape) == 2\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "#word_embeddings = Embedding(len(vocab), embedding_vector_length, input_length=max_review_length)\n",
    "#model.add(word_embeddings)\n",
    "# model.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))\n",
    "# input shape is a pair of (timesteps, features) <https://stackoverflow.com/a/44583784/1391325>\n",
    "input_shape = training_x.shape[1:3]\n",
    "print(\"Input shape: {}\".format(input_shape), file=sys.stderr)\n",
    "units = training_y.shape[1]\n",
    "print(\"Units: {}\".format(units), file=sys.stderr)\n",
    "lstm = LSTM(input_shape=input_shape, units=units)\n",
    "#lstm = LSTM(batch_input_shape = training_x.shape, stateful = True, units=len(training_y.shape))\n",
    "model.add(lstm)\n",
    "model.add(Dense(units, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6843 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6830 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6812 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6843 - acc: 0.5000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6841 - acc: 0.5000\n"
     ]
    }
   ],
   "source": [
    "# train LSTM\n",
    "#model.fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None)\n",
    "#training_eval = model.fit(x=training_x, y=training_y, epochs=1, verbose=1)\n",
    "\n",
    "# train LSTM <https://machinelearningmastery.com/memory-in-a-long-short-term-memory-network/>\n",
    "epochs = 5\n",
    "for i in range(epochs):\n",
    "    x = training_x[i]\n",
    "    x_oldshape = x.shape\n",
    "    x_newshape = (-1, x_oldshape[0], x_oldshape[1])\n",
    "    x = np.reshape(x, x_newshape)\n",
    "    #print(\"x.shape = {}\".format(x.shape), file=sys.stderr)\n",
    "    y = training_y[i]\n",
    "    y_oldshape = y.shape\n",
    "    y_newshape = (-1, y_oldshape[0])\n",
    "    y = np.reshape(y, y_newshape)\n",
    "    #print(\"y.shape = {}\".format(y.shape), file=sys.stderr)\n",
    "    model.fit(x, y, epochs=1, batch_size=1, verbose=1, shuffle=False)\n",
    "    model.reset_states()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
