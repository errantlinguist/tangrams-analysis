{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will read file \"D:\\Users\\tcshore\\Documents\\Projects\\Tangrams\\Data\\wordprobseqs-inflected-small.tsv\".\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "import sys\n",
    "\n",
    "def get_project_root() -> str:\n",
    "    system = platform.system()\n",
    "    return r\"D:\\Users\\tcshore\\Documents\\Projects\\Tangrams\\Data\" if system == \"Windows\" else \"/home/tshore/Projects/tangrams-restricted/Data\"\n",
    "\n",
    "infile_path = os.path.join(get_project_root(), \"wordprobseqs-inflected-small.tsv\")\n",
    "print(\"Will read file \\\"{}\\\".\".format(infile_path), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading \"D:\\Users\\tcshore\\Documents\\Projects\\Tangrams\\Data\\wordprobseqs-inflected-small.tsv\" using encoding \"windows-1252\".\n",
      "Read 180880 cross-validation results for 2 dyad(s).\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "RESULTS_FILE_CSV_DIALECT = csv.excel_tab\n",
    "# NOTE: \"category\" dtype doesn't work with pandas-0.21.0 but does with pandas-0.21.1\n",
    "__RESULTS_FILE_DTYPES = {\"DYAD\": \"category\", \"WORD\": \"category\", \"IS_TARGET\": bool, \"IS_OOV\": bool,\n",
    "\t\t\t\t \"IS_INSTRUCTOR\": bool, \"SHAPE\": \"category\", \"ONLY_INSTRUCTOR\": bool, \"WEIGHT_BY_FREQ\": bool}\n",
    "\n",
    "def read_results_file(inpath: str, encoding: str) -> pd.DataFrame:\n",
    "\tprint(\"Reading \\\"{}\\\" using encoding \\\"{}\\\".\".format(inpath, encoding), file=sys.stderr)\n",
    "\tresult = pd.read_csv(inpath, dialect=RESULTS_FILE_CSV_DIALECT, sep=RESULTS_FILE_CSV_DIALECT.delimiter,\n",
    "\t\t\t\t\t\t float_precision=\"round_trip\",\n",
    "\t\t\t\t\t\t encoding=encoding, memory_map=True, dtype=__RESULTS_FILE_DTYPES)\n",
    "\treturn result\n",
    "\n",
    "cv_results = read_results_file(infile_path, \"windows-1252\")\n",
    "print(\"Read {} cross-validation results for {} dyad(s).\".format(cv_results.shape[0], cv_results[\"DYAD\"].nunique()),\n",
    "      file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found 171836 nontarget rows and 9044 target rows. Ratio: 19.0\n"
     ]
    }
   ],
   "source": [
    "def find_target_ref_rows(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\tresult = df.loc[df[\"IS_TARGET\"] == True]\n",
    "\tresult_row_count = result.shape[0]\n",
    "\tcomplement_row_count = df.loc[~df.index.isin(result.index)].shape[0]\n",
    "\tassert result_row_count + complement_row_count == df.shape[0]\n",
    "\tprint(\"Found {} nontarget rows and {} target rows. Ratio: {}\".format(complement_row_count, result_row_count,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t complement_row_count / float(\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t result_row_count)), file=sys.stderr)\n",
    "\treturn result\n",
    "\n",
    "cv_results = find_target_ref_rows(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting one-hot encoder for vocabulary of size 299.\n",
      "Creating vocab dictionary for one-hot label encoding.\n"
     ]
    }
   ],
   "source": [
    "# Create vocab before splitting training and testing DFs so that the word feature set is stable\n",
    "vocab = tuple(sorted(cv_results[\"WORD\"].unique()))\n",
    "print(\"Fitting one-hot encoder for vocabulary of size {}.\".format(len(vocab)), file=sys.stderr)\n",
    "# https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/\n",
    "print(\"Creating vocab dictionary for one-hot label encoding.\", file=sys.stderr)\n",
    "vocab_idxs = dict((word, idx) for (idx, word) in enumerate(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training set dyads: ['17']\n",
      "Test set dyads: ['1']\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def split_training_testing(df: pd.DataFrame, test_set_size : int) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "\tdyad_ids = df[\"DYAD\"].unique()\n",
    "\ttraining_set_size = len(dyad_ids) - test_set_size\n",
    "\tif training_set_size < 1:\n",
    "\t\traise ValueError(\"Desired test set size is {} but only {} dyads found.\".format(test_set_size, len(dyad_ids)))\n",
    "\telse:\n",
    "\t\ttraining_set_dyads = frozenset(np.random.choice(dyad_ids, training_set_size))\n",
    "\t\tprint(\"Training set dyads: {}\".format(sorted(training_set_dyads)), file=sys.stderr)\n",
    "\t\ttraining_set_idxs = df[\"DYAD\"].isin(training_set_dyads)\n",
    "\t\ttraining_set = df.loc[training_set_idxs]\n",
    "\t\ttest_set = df.loc[~training_set_idxs]\n",
    "\t\ttest_set_dyads = frozenset(test_set[\"DYAD\"].unique())\n",
    "\t\tprint(\"Test set dyads: {}\".format(sorted(test_set_dyads)), file=sys.stderr)\n",
    "\n",
    "\t\tassert not frozenset(training_set[\"DYAD\"].unique()).intersection(frozenset(test_set_dyads))\n",
    "\t\treturn training_set, test_set\n",
    "    \n",
    "training_df, test_df = split_training_testing(cv_results, 1)\n",
    "training_df = training_df.copy(deep=False)\n",
    "test_df = test_df.copy(deep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting token sequences.\n",
      "Created a training data matrix of shape (1174, 4, 302).\n",
      "Created a test data matrix of shape (1087, 4, 302).\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterator, List, Mapping\n",
    "\n",
    "class SequenceMatrixFactory(object):\n",
    "\n",
    "\tdef __init__(self,vocab_idxs : Mapping[str, int]):\n",
    "\t\tself.__vocab_idxs = vocab_idxs\n",
    "\t\tself.__feature_count = len(self.__vocab_idxs) + 3\n",
    "\n",
    "\tdef create_datapoint_feature_array(self, row: pd.Series) -> List[float]:\n",
    "\t\tword_features = [0.0] * len(self.__vocab_idxs)\n",
    "\t\t# The features representing each individual vocabulary word are at the beginning of the feature vector\n",
    "\t\tword_features[self.__vocab_idxs[row[\"WORD\"]]] = 1.0\n",
    "\t\tis_instructor = 1.0 if row[\"IS_INSTRUCTOR\"] else 0.0\n",
    "\t\tis_oov = 1.0 if row[\"IS_OOV\"] else 0.0\n",
    "\t\t#is_target = 1.0 if row[\"IS_TARGET\"] else 0.0\n",
    "\t\tscore = row[\"PROBABILITY\"]\n",
    "\t\tother_features = list((is_instructor, is_oov, score))\n",
    "\t\tresult = word_features + other_features\n",
    "\t\t#print(\"Created a vector of {} features.\".format(len(result)), file=sys.stderr)\n",
    "\t\treturn result\n",
    "\n",
    "\tdef __call__(self, df : pd.DataFrame) -> np.array:\n",
    "\t\t# https://stackoverflow.com/a/47815400/1391325\n",
    "\t\tdf.sort_values(\"TOKEN_SEQ_ORDINALITY\", inplace=True)\n",
    "\t\tsequence_groups = df.groupby((\"CROSS_VALIDATION_ITER\", \"DYAD\", \"SPLIT_SEQ_NO\", \"UTT_START_TIME\", \"UTT_END_TIME\", \"ENTITY\"), as_index=False)\n",
    "\t\treturn np.array(tuple(tuple(self.__create_feature_vectors(seq)) for _, seq in sequence_groups))\n",
    "        \n",
    "\tdef __create_feature_vectors(self, df : pd.DataFrame) -> Iterator[List[float]]:\n",
    "\t\t# noinspection PyProtectedMember\n",
    "\t\treturn (self.create_datapoint_feature_array(row._asdict()) for row in df.itertuples(index=False))\n",
    "\n",
    "print(\"Splitting token sequences.\", file=sys.stderr)\n",
    "seq_matrix_factory = SequenceMatrixFactory(vocab_idxs)\n",
    "training_matrix = seq_matrix_factory(training_df)\n",
    "print(\"Created a training data matrix of shape {}.\".format(training_matrix.shape), file=sys.stderr)\n",
    "test_matrix = seq_matrix_factory(test_df)\n",
    "print(\"Created a test data matrix of shape {}.\".format(test_matrix.shape), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1174, 4, 301)\n",
      "(1174, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input shape: (4, 301)\n",
      "Units: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 4)                 4896      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 20        \n",
      "=================================================================\n",
      "Total params: 4,916\n",
      "Trainable params: 4,916\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "training_x = training_matrix[:,:,:-1]\n",
    "print(training_x.shape)\n",
    "assert len(training_x.shape) == 3\n",
    "training_y = training_matrix[:,:,-1]\n",
    "print(training_y.shape)\n",
    "assert len(training_y.shape) == 2\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "#word_embeddings = Embedding(len(vocab), embedding_vector_length, input_length=max_review_length)\n",
    "#model.add(word_embeddings)\n",
    "# model.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))\n",
    "# input shape is a pair of (timesteps, features) <https://stackoverflow.com/a/44583784/1391325>\n",
    "input_shape = training_x.shape[1:3]\n",
    "print(\"Input shape: {}\".format(input_shape), file=sys.stderr)\n",
    "units = training_y.shape[1]\n",
    "print(\"Units: {}\".format(units), file=sys.stderr)\n",
    "lstm = LSTM(input_shape=input_shape, units=units)\n",
    "#lstm = LSTM(batch_input_shape = training_x.shape, stateful = True, units=len(training_y.shape))\n",
    "model.add(lstm)\n",
    "model.add(Dense(units, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1174/1174 [==============================] - 1s 704us/step - loss: 0.6913 - acc: 0.1033\n"
     ]
    }
   ],
   "source": [
    "# train LSTM\n",
    "#model.fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None)\n",
    "training_eval = model.fit(x=training_x, y=training_y, epochs=1, verbose=1)\n",
    "\n",
    "# train LSTM <https://machinelearningmastery.com/memory-in-a-long-short-term-memory-network/>\n",
    "#epochs = 5\n",
    "#for i in range(epochs):\n",
    "#    x = training_x[i]\n",
    "#    x_oldshape = x.shape\n",
    "#    x_newshape = (-1, x_oldshape[0], x_oldshape[1])\n",
    "#    x = np.reshape(x, x_newshape)\n",
    "#    #print(\"x.shape = {}\".format(x.shape), file=sys.stderr)\n",
    "#    y = training_y[i]\n",
    "#    y_oldshape = y.shape\n",
    "#   y_newshape = (-1, y_oldshape[0])\n",
    "#    y = np.reshape(y, y_newshape)\n",
    "#    #print(\"y.shape = {}\".format(y.shape), file=sys.stderr)\n",
    "#    model.fit(x, y, epochs=1, batch_size=1, verbose=1, shuffle=False)\n",
    "#    model.reset_states()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1087, 4, 301)\n",
      "(1087, 4)\n",
      "[ 0.46308592  0.45843136  0.48473114  0.49586362]\n",
      "[ 0.46957338  0.46836948  0.48374063  0.49399155]\n",
      "[ 0.47560182  0.45543796  0.50316155  0.51140511]\n",
      "[ 0.46666119  0.46612591  0.48648214  0.49517879]\n",
      "[ 0.47584179  0.46816602  0.49215242  0.50245005]\n",
      "[ 0.47448283  0.46105126  0.49357796  0.50371981]\n",
      "[ 0.47684219  0.46265781  0.50063604  0.51067114]\n",
      "[ 0.48283237  0.46161124  0.49825132  0.51118737]\n",
      "[ 0.47392333  0.46753961  0.48109448  0.4924449 ]\n",
      "[ 0.47446534  0.469201    0.49714205  0.50755835]\n",
      "[ 0.46899396  0.46285978  0.49625012  0.50418288]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.47560182  0.45543796  0.50316155  0.51140511]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.48370403  0.4582901   0.49072936  0.5014689 ]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.48214042  0.46293092  0.4941169   0.50605404]\n",
      "[ 0.47486231  0.46335119  0.50397515  0.5134595 ]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.47446534  0.469201    0.49714205  0.50755835]\n",
      "[ 0.46899396  0.46285978  0.49625012  0.50418288]\n",
      "[ 0.46899396  0.46285978  0.49625012  0.50418288]\n",
      "[ 0.47570929  0.46640399  0.49666286  0.50509959]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.47382161  0.45723733  0.48502555  0.49401164]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.48214042  0.46293092  0.4941169   0.50605404]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.47560182  0.45543796  0.50316155  0.51140511]\n",
      "[ 0.46666119  0.46612591  0.48648214  0.49517879]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.47486231  0.46335119  0.50397515  0.5134595 ]\n",
      "[ 0.46899396  0.46285978  0.49625012  0.50418288]\n",
      "[ 0.48173636  0.46715215  0.50013202  0.50902271]\n",
      "[ 0.48173636  0.46715215  0.50013202  0.50902271]\n",
      "[ 0.4703005   0.45836723  0.49999082  0.50834674]\n",
      "[ 0.4729149   0.46510568  0.49270651  0.50420874]\n",
      "[ 0.47448283  0.46105126  0.49357796  0.50371981]\n",
      "[ 0.47956684  0.46591336  0.48777577  0.49851555]\n",
      "[ 0.47446534  0.469201    0.49714205  0.50755835]\n",
      "[ 0.47448283  0.46105126  0.49357796  0.50371981]\n",
      "[ 0.47152463  0.46767375  0.49015468  0.49919534]\n",
      "[ 0.47344515  0.46586248  0.48835781  0.50011992]\n",
      "[ 0.48370403  0.4582901   0.49072936  0.5014689 ]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.47560182  0.45543796  0.50316155  0.51140511]\n",
      "[ 0.4718585   0.45755109  0.498436    0.50669146]\n",
      "[ 0.47446534  0.469201    0.49714205  0.50755835]\n",
      "[ 0.48369443  0.46455175  0.49474317  0.50762242]\n",
      "[ 0.47446534  0.469201    0.49714205  0.50755835]\n",
      "[ 0.47956684  0.46591336  0.48777577  0.49851555]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.47956684  0.46591336  0.48777577  0.49851555]\n",
      "[ 0.48214042  0.46293092  0.4941169   0.50605404]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.47859532  0.46386003  0.49464765  0.50790632]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.46778256  0.45406994  0.50493449  0.51299834]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.47448283  0.46105126  0.49357796  0.50371981]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.47392333  0.46753961  0.48109448  0.4924449 ]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.46957546  0.46877968  0.48596245  0.49722633]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.46308592  0.45843136  0.48473114  0.49586362]\n",
      "[ 0.47392333  0.46753961  0.48109448  0.4924449 ]\n",
      "[ 0.47956684  0.46591336  0.48777577  0.49851555]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.47446534  0.469201    0.49714205  0.50755835]\n",
      "[ 0.46778256  0.45406994  0.50493449  0.51299834]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.47956684  0.46591336  0.48777577  0.49851555]\n",
      "[ 0.47446534  0.469201    0.49714205  0.50755835]\n",
      "[ 0.48214042  0.46293092  0.4941169   0.50605404]\n",
      "[ 0.47716042  0.46380115  0.49205333  0.50313151]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.47352263  0.45791194  0.49095717  0.5028457 ]\n",
      "[ 0.48369443  0.46455175  0.49474317  0.50762242]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.48135743  0.46032578  0.4997499   0.51061141]\n",
      "[ 0.47584179  0.46816602  0.49215242  0.50245005]\n",
      "[ 0.47956684  0.46591336  0.48777577  0.49851555]\n",
      "[ 0.46899396  0.46285978  0.49625012  0.50418288]\n",
      "[ 0.47934934  0.46550697  0.49082536  0.50382006]\n",
      "[ 0.46957546  0.46877968  0.48596245  0.49722633]\n",
      "[ 0.46666119  0.46612591  0.48648214  0.49517879]\n",
      "[ 0.46957546  0.46877968  0.48596245  0.49722633]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.47446534  0.469201    0.49714205  0.50755835]\n",
      "[ 0.47560182  0.45543796  0.50316155  0.51140511]\n",
      "[ 0.46666119  0.46612591  0.48648214  0.49517879]\n",
      "[ 0.47570929  0.46640399  0.49666286  0.50509959]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.47446534  0.469201    0.49714205  0.50755835]\n",
      "[ 0.46666119  0.46612591  0.48648214  0.49517879]\n",
      "[ 0.47716042  0.46380115  0.49205333  0.50313151]\n",
      "[ 0.48214042  0.46293092  0.4941169   0.50605404]\n",
      "[ 0.46666119  0.46612591  0.48648214  0.49517879]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.46899396  0.46285978  0.49625012  0.50418288]\n",
      "[ 0.48370403  0.4582901   0.49072936  0.5014689 ]\n",
      "[ 0.46899396  0.46285978  0.49625012  0.50418288]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.46666119  0.46612591  0.48648214  0.49517879]\n",
      "[ 0.46666119  0.46612591  0.48648214  0.49517879]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.47956684  0.46591336  0.48777577  0.49851555]\n",
      "[ 0.48370403  0.4582901   0.49072936  0.5014689 ]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.46666119  0.46612591  0.48648214  0.49517879]\n",
      "[ 0.46666119  0.46612591  0.48648214  0.49517879]\n",
      "[ 0.4790954   0.46221119  0.49970144  0.51094306]\n",
      "[ 0.46666119  0.46612591  0.48648214  0.49517879]\n",
      "[ 0.46666119  0.46612591  0.48648214  0.49517879]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.47570929  0.46640399  0.49666286  0.50509959]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.47375497  0.46625602  0.48905495  0.50002152]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.47446534  0.469201    0.49714205  0.50755835]\n",
      "[ 0.46706244  0.46552575  0.49699926  0.505247  ]\n",
      "[ 0.47560182  0.45543796  0.50316155  0.51140511]\n",
      "[ 0.46666119  0.46612591  0.48648214  0.49517879]\n",
      "[ 0.46666119  0.46612591  0.48648214  0.49517879]\n",
      "[ 0.46666119  0.46612591  0.48648214  0.49517879]\n",
      "[ 0.46666119  0.46612591  0.48648214  0.49517879]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.4703005   0.45836723  0.49999082  0.50834674]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.47448283  0.46105126  0.49357796  0.50371981]\n",
      "[ 0.48370403  0.4582901   0.49072936  0.5014689 ]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.47716042  0.46380115  0.49205333  0.50313151]\n",
      "[ 0.46706244  0.46552575  0.49699926  0.505247  ]\n",
      "[ 0.47570929  0.46640399  0.49666286  0.50509959]\n",
      "[ 0.47158077  0.46062785  0.49981791  0.50732964]\n",
      "[ 0.47448283  0.46105126  0.49357796  0.50371981]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.48369443  0.46455175  0.49474317  0.50762242]\n",
      "[ 0.46666119  0.46612591  0.48648214  0.49517879]\n",
      "[ 0.46778256  0.45406994  0.50493449  0.51299834]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.46666119  0.46612591  0.48648214  0.49517879]\n",
      "[ 0.46666119  0.46612591  0.48648214  0.49517879]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.4695445   0.46245214  0.4904584   0.49987319]\n",
      "[ 0.46706244  0.46552575  0.49699926  0.505247  ]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.47595403  0.46570107  0.48912403  0.49884385]\n",
      "[ 0.47158077  0.46062785  0.49981791  0.50732964]\n",
      "[ 0.47560182  0.45543796  0.50316155  0.51140511]\n",
      "[ 0.46666119  0.46612591  0.48648214  0.49517879]\n",
      "[ 0.46666119  0.46612591  0.48648214  0.49517879]\n",
      "[ 0.46666119  0.46612591  0.48648214  0.49517879]\n",
      "[ 0.48214042  0.46293092  0.4941169   0.50605404]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.47158077  0.46062785  0.49981791  0.50732964]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.47716042  0.46380115  0.49205333  0.50313151]\n",
      "[ 0.46899396  0.46285978  0.49625012  0.50418288]\n",
      "[ 0.46666119  0.46612591  0.48648214  0.49517879]\n",
      "[ 0.46706244  0.46552575  0.49699926  0.505247  ]\n",
      "[ 0.46901709  0.4648059   0.48772246  0.49767613]\n",
      "[ 0.47448283  0.46105126  0.49357796  0.50371981]\n",
      "[ 0.47448283  0.46105126  0.49357796  0.50371981]\n",
      "[ 0.47207975  0.46237576  0.49701586  0.50707829]\n",
      "[ 0.47480974  0.47188112  0.47565252  0.48549944]\n",
      "[ 0.48255575  0.45322013  0.50986636  0.51641846]\n",
      "[ 0.46962816  0.46923879  0.49385622  0.50250268]\n",
      "[ 0.48191988  0.47311506  0.49147198  0.50130123]\n",
      "[ 0.47991988  0.45767701  0.49996686  0.50855815]\n",
      "[ 0.49134424  0.46927559  0.48887905  0.50050062]\n",
      "[ 0.47973073  0.47492296  0.48343205  0.4946427 ]\n",
      "[ 0.46820363  0.46797681  0.48800552  0.49471661]\n",
      "[ 0.48255575  0.45322013  0.50986636  0.51641846]\n",
      "[ 0.49128264  0.46166691  0.49250853  0.50255138]\n",
      "[ 0.47398022  0.46102458  0.49082837  0.49860764]\n",
      "[ 0.47971231  0.46661943  0.49031779  0.50301516]\n",
      "[ 0.48175704  0.46621633  0.50109053  0.51063871]\n",
      "[ 0.47971231  0.46661943  0.49031779  0.50301516]\n",
      "[ 0.47973073  0.47492296  0.48343205  0.4946427 ]\n",
      "[ 0.47709399  0.45717824  0.50466436  0.5110119 ]\n",
      "[ 0.47599399  0.46660873  0.47958931  0.48866796]\n",
      "[ 0.48845676  0.46584433  0.49833012  0.50721216]\n",
      "[ 0.46734878  0.46198657  0.48794767  0.49613371]\n",
      "[ 0.47828832  0.4532932   0.47885782  0.48634681]\n",
      "[ 0.49412256  0.46517846  0.49469286  0.50733709]\n",
      "[ 0.47392333  0.46367154  0.49359652  0.50053102]\n",
      "[ 0.46962816  0.46923879  0.49385622  0.50250268]\n",
      "[ 0.4850519   0.46728641  0.49037439  0.50267607]\n",
      "[ 0.47047919  0.46661776  0.48934349  0.49729118]\n",
      "[ 0.48267713  0.45751345  0.49747992  0.50680846]\n",
      "[ 0.48693702  0.47337905  0.49122903  0.5012126 ]\n",
      "[ 0.48473328  0.46138102  0.4957397   0.50683421]\n",
      "[ 0.47759062  0.47407344  0.4821375   0.49380434]\n",
      "[ 0.4870235   0.4623245   0.49327645  0.50453645]\n",
      "[ 0.47973073  0.47492296  0.48343205  0.4946427 ]\n",
      "[ 0.48917115  0.45778051  0.49591404  0.50572741]\n",
      "[ 0.47964779  0.46510923  0.49015749  0.50091326]\n",
      "[ 0.48600119  0.46335015  0.48556143  0.4969146 ]\n",
      "[ 0.48338559  0.46553302  0.4899621   0.5013476 ]\n",
      "[ 0.47893918  0.45815372  0.50658274  0.51249135]\n",
      "[ 0.47237197  0.46285713  0.49106759  0.49916443]\n",
      "[ 0.47973073  0.47492296  0.48343205  0.4946427 ]\n",
      "[ 0.47973073  0.47492296  0.48343205  0.4946427 ]\n",
      "[ 0.47590488  0.46962002  0.48223606  0.49087971]\n",
      "[ 0.47685     0.45747992  0.5007453   0.50876606]\n",
      "[ 0.48160878  0.46977979  0.48198241  0.49197832]\n",
      "[ 0.47008386  0.46738723  0.48362941  0.49302468]\n",
      "[ 0.48076412  0.46206158  0.50372446  0.51231503]\n",
      "[ 0.46773583  0.46871662  0.48236305  0.49049875]\n",
      "[ 0.47848862  0.46166068  0.50098544  0.5109266 ]\n",
      "[ 0.46876073  0.45662278  0.50090647  0.50841421]\n",
      "[ 0.469715    0.46167099  0.50658506  0.51456016]\n",
      "[ 0.47008386  0.46738723  0.48362941  0.49302468]\n",
      "[ 0.47008386  0.46738723  0.48362941  0.49302468]\n",
      "[ 0.47732592  0.46404561  0.48936206  0.49820709]\n",
      "[ 0.4745594   0.46262404  0.48628315  0.49434   ]\n",
      "[ 0.47745776  0.45946613  0.49018428  0.49903014]\n",
      "[ 0.47207975  0.46237576  0.49701586  0.50707829]\n",
      "[ 0.48086295  0.4597078   0.49464458  0.50345963]\n",
      "[ 0.4838264   0.46458206  0.48901442  0.49900511]\n",
      "[ 0.47973073  0.47492296  0.48343205  0.4946427 ]\n",
      "[ 0.47415248  0.46868509  0.50252223  0.50957537]\n",
      "[ 0.4829036   0.46974361  0.48493531  0.49582681]\n",
      "[ 0.47973073  0.47492296  0.48343205  0.4946427 ]\n",
      "[ 0.49033567  0.46529111  0.50078714  0.5114575 ]\n",
      "[ 0.48542523  0.46578965  0.49315882  0.50355136]\n",
      "[ 0.47159922  0.46286806  0.49165615  0.49901134]\n",
      "[ 0.48104191  0.4682028   0.48878857  0.49991518]\n",
      "[ 0.46644258  0.46562693  0.47612363  0.48605701]\n",
      "[ 0.47746786  0.46430472  0.49780798  0.50447899]\n",
      "[ 0.47285348  0.4711656   0.48581159  0.49407709]\n",
      "[ 0.4847348   0.45893127  0.4992393   0.50747579]\n",
      "[ 0.48398772  0.46499157  0.49166551  0.50331777]\n",
      "[ 0.46843231  0.47202009  0.4808678   0.49005955]\n",
      "[ 0.46962816  0.46923879  0.49385622  0.50250268]\n",
      "[ 0.46843231  0.47202009  0.4808678   0.49005955]\n",
      "[ 0.4850519   0.46728641  0.49037439  0.50267607]\n",
      "[ 0.47973073  0.47492296  0.48343205  0.4946427 ]\n",
      "[ 0.48465228  0.46120477  0.49906749  0.50845748]\n",
      "[ 0.46962816  0.46923879  0.49385622  0.50250268]\n",
      "[ 0.48963118  0.46362847  0.49907711  0.50858521]\n",
      "[ 0.47347015  0.46285245  0.49002612  0.49739313]\n",
      "[ 0.46773583  0.46871662  0.48236305  0.49049875]\n",
      "[ 0.47650573  0.47076702  0.48223627  0.4929533 ]\n",
      "[ 0.47973073  0.47492296  0.48343205  0.4946427 ]\n",
      "[ 0.46962816  0.46923879  0.49385622  0.50250268]\n",
      "[ 0.48286617  0.46682012  0.49180698  0.50148398]\n",
      "[ 0.482555    0.46988702  0.48803586  0.49786764]\n",
      "[ 0.46962816  0.46923879  0.49385622  0.50250268]\n",
      "[ 0.48076412  0.46206158  0.50372446  0.51231503]\n",
      "[ 0.47312737  0.46245837  0.50104696  0.50671804]\n",
      "[ 0.48591048  0.46334034  0.4871603   0.49776614]\n",
      "[ 0.46914309  0.45579469  0.50651145  0.5126    ]\n",
      "[ 0.46773583  0.46871662  0.48236305  0.49049875]\n",
      "[ 0.46962816  0.46923879  0.49385622  0.50250268]\n",
      "[ 0.48129141  0.47085437  0.48225179  0.49236143]\n",
      "[ 0.47837496  0.46635753  0.49174637  0.4995909 ]\n",
      "[ 0.49165523  0.45673546  0.50562304  0.51442552]\n",
      "[ 0.47817081  0.46648943  0.48780653  0.49832892]\n",
      "[ 0.46962816  0.46923879  0.49385622  0.50250268]\n",
      "[ 0.47072607  0.47272494  0.4821232   0.48993585]\n",
      "[ 0.48937696  0.46846706  0.49529347  0.50875241]\n",
      "[ 0.46962816  0.46923879  0.49385622  0.50250268]\n",
      "[ 0.46962816  0.46923879  0.49385622  0.50250268]\n",
      "[ 0.47008386  0.46738723  0.48362941  0.49302468]\n",
      "[ 0.48161763  0.46679071  0.49901381  0.50706625]\n",
      "[ 0.47533271  0.46434346  0.49033648  0.49851063]\n",
      "[ 0.47973073  0.47492296  0.48343205  0.4946427 ]\n",
      "[ 0.48465228  0.46120477  0.49906749  0.50845748]\n",
      "[ 0.46962816  0.46923879  0.49385622  0.50250268]\n",
      "[ 0.46962816  0.46923879  0.49385622  0.50250268]\n",
      "[ 0.46962816  0.46923879  0.49385622  0.50250268]\n",
      "[ 0.46962816  0.46923879  0.49385622  0.50250268]\n",
      "[ 0.47695836  0.47078821  0.4880358   0.4977749 ]\n",
      "[ 0.47548392  0.4622176   0.49844554  0.50847131]\n",
      "[ 0.48963609  0.46409562  0.48710322  0.49835226]\n",
      "[ 0.48696992  0.45861772  0.49109432  0.50082964]\n",
      "[ 0.46773583  0.46871662  0.48236305  0.49049875]"
     ]
    }
   ],
   "source": [
    "# test LSTM\n",
    "\n",
    "\n",
    "test_x = test_matrix[:,:,:-1]\n",
    "print(test_x.shape)\n",
    "assert len(test_x.shape) == 3\n",
    "test_y = test_matrix[:,:,-1]\n",
    "print(test_y.shape)\n",
    "assert len(test_y.shape) == 2\n",
    "\n",
    "result = model.predict(test_x, verbose=0)\n",
    "model.reset_states()\n",
    "print(\"result.shape = {}\".format(result.shape), file=sys.stderr)\n",
    "for i, datum in enumerate(result):\n",
    "    #print(i)\n",
    "    print(datum)\n",
    "\t#print('X=%.1f y=%.1f, yhat=%.1f' % (seq1[i], seq1[i+1], result[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
