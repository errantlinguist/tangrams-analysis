{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will read file \"D:\\Users\\tcshore\\Documents\\Projects\\Tangrams\\Data\\wordscores-inflected-small.tsv\".\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "import sys\n",
    "\n",
    "def get_project_root() -> str:\n",
    "    system = platform.system()\n",
    "    return r\"D:\\Users\\tcshore\\Documents\\Projects\\Tangrams\\Data\" if system == \"Windows\" else \"/home/tshore/Projects/tangrams-restricted/Data\"\n",
    "\n",
    "infile_path = os.path.join(get_project_root(), \"wordscores-inflected-small.tsv\")\n",
    "print(\"Will read file \\\"{}\\\".\".format(infile_path), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading \"D:\\Users\\tcshore\\Documents\\Projects\\Tangrams\\Data\\wordscores-inflected-small.tsv\" using encoding \"windows-1252\".\n",
      "Read 67060 cross-validation results for 3 dyad(s).\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "RESULTS_FILE_CSV_DIALECT = csv.excel_tab\n",
    "# NOTE: \"category\" dtype doesn't work with pandas-0.21.0 but does with pandas-0.21.1\n",
    "__RESULTS_FILE_DTYPES = {\"DYAD\": \"category\", \"ENTITY\" : \"category\", \"IS_TARGET\": bool, \"IS_OOV\": bool,\n",
    "\t\t\t\t \"IS_INSTRUCTOR\": bool, \"SHAPE\": \"category\", \"ONLY_INSTRUCTOR\": bool, \"WEIGHT_BY_FREQ\": bool}\n",
    "\n",
    "def read_results_file(inpath: str, encoding: str) -> pd.DataFrame:\n",
    "\tprint(\"Reading \\\"{}\\\" using encoding \\\"{}\\\".\".format(inpath, encoding), file=sys.stderr)\n",
    "\tresult = pd.read_csv(inpath, dialect=RESULTS_FILE_CSV_DIALECT, sep=RESULTS_FILE_CSV_DIALECT.delimiter,\n",
    "\t\t\t\t\t\t float_precision=\"round_trip\",\n",
    "\t\t\t\t\t\t encoding=encoding, memory_map=True, dtype=__RESULTS_FILE_DTYPES)\n",
    "\treturn result\n",
    "\n",
    "cv_results = read_results_file(infile_path, \"windows-1252\")\n",
    "print(\"Read {} cross-validation results for {} dyad(s).\".format(cv_results.shape[0], cv_results[\"DYAD\"].nunique()),\n",
    "      file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found 63707 nontarget rows and 3353 target rows. Ratio: 19.0\n"
     ]
    }
   ],
   "source": [
    "def find_target_ref_rows(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\tresult = df.loc[df[\"IS_TARGET\"] == True]\n",
    "\tresult_row_count = result.shape[0]\n",
    "\tcomplement_row_count = df.loc[~df.index.isin(result.index)].shape[0]\n",
    "\tassert result_row_count + complement_row_count == df.shape[0]\n",
    "\tprint(\"Found {} nontarget rows and {} target rows. Ratio: {}\".format(complement_row_count, result_row_count,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t complement_row_count / float(\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t result_row_count)), file=sys.stderr)\n",
    "\treturn result\n",
    "\n",
    "cv_results = find_target_ref_rows(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Replacing OOV words with label \"__OOV__\".\n"
     ]
    }
   ],
   "source": [
    "OOV_LABEL = \"__OOV__\"\n",
    "    \n",
    "print(\"Replacing OOV words with label \\\"{}\\\".\".format(OOV_LABEL), file=sys.stderr)\n",
    "cv_results.loc[cv_results[\"IS_OOV\"] == True, \"WORD\"] = OOV_LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting one-hot encoder for vocabulary of size 249.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#import numpy as np\n",
    "\n",
    "# Create vocab before splitting training and testing DFs so that the word feature set is stable\n",
    "print(\"Fitting one-hot encoder for vocabulary of size {}.\".format(cv_results[\"WORD\"].nunique()), file=sys.stderr)\n",
    "\n",
    "# https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "vocab_labels = label_encoder.fit_transform(cv_results[\"WORD\"])\n",
    "cv_results[\"WORD_LABEL\"] = vocab_labels\n",
    "#print(vocab_labels)\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "vocab_labels = vocab_labels.reshape(len(vocab_labels), 1)\n",
    "onehot_encoder.fit(vocab_labels)\n",
    "#assert onehot_encoder.n_values_ == len(vocab_words)\n",
    "#vocab_onehot_encoded = onehot_encoder.fit_transform(vocab_labels)\n",
    "#print(vocab_onehot_encoded)\n",
    "# invert first example\n",
    "#inverted = label_encoder.inverse_transform([np.argmax(vocab_onehot_encoded[0, :])])\n",
    "#print(inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training set dyads: ['20', '22']\n",
      "Test set dyads: ['21']\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def split_training_testing(df: pd.DataFrame, test_set_size : int) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "\tdyad_ids = df[\"DYAD\"].unique()\n",
    "\ttraining_set_size = len(dyad_ids) - test_set_size\n",
    "\tif training_set_size < 1:\n",
    "\t\traise ValueError(\"Desired test set size is {} but only {} dyads found.\".format(test_set_size, len(dyad_ids)))\n",
    "\telse:\n",
    "\t\ttraining_set_dyads = frozenset(np.random.choice(dyad_ids, training_set_size))\n",
    "\t\tprint(\"Training set dyads: {}\".format(sorted(training_set_dyads)), file=sys.stderr)\n",
    "\t\ttraining_set_idxs = df[\"DYAD\"].isin(training_set_dyads)\n",
    "\t\ttraining_set = df.loc[training_set_idxs]\n",
    "\t\ttest_set = df.loc[~training_set_idxs]\n",
    "\t\ttest_set_dyads = frozenset(test_set[\"DYAD\"].unique())\n",
    "\t\tprint(\"Test set dyads: {}\".format(sorted(test_set_dyads)), file=sys.stderr)\n",
    "\n",
    "\t\tassert not frozenset(training_set[\"DYAD\"].unique()).intersection(frozenset(test_set_dyads))\n",
    "\t\treturn training_set, test_set\n",
    "    \n",
    "# https://stackoverflow.com/a/47815400/1391325\n",
    "cv_results.sort_values(\"TOKEN_SEQ_ORDINALITY\", inplace=True)\n",
    "training_df, test_df = split_training_testing(cv_results, 1)\n",
    "#training_df = training_df.copy(deep=False)\n",
    "#test_df = test_df.copy(deep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting token sequences.\n",
      "Created a training dataset with a size of 291 and a max sequence length of 40.\n",
      "Created a test dataset with a size of 125 and a max sequence length of 33.\n",
      "Padding sequences to a length of 40.\n",
      "Batch training matrix shape: (291, 40, 251)\n",
      "Batch test matrix shape: (125, 40, 251)\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterator, List, Mapping\n",
    "\n",
    "import keras.preprocessing.sequence\n",
    "\n",
    "class SequenceMatrixFactory(object):\n",
    "\n",
    "\tdef __init__(self, onehot_encoder):\n",
    "\t\tself.onehot_encoder = onehot_encoder\n",
    "\n",
    "\t@property\n",
    "\tdef feature_count(self) -> int:\n",
    "\t\tword_features = self.onehot_encoder.n_values_[0]\n",
    "\t\treturn word_features + 2\n",
    "\n",
    "\tdef __create_datapoint_feature_array(self, row: pd.Series) -> np.array:\n",
    "\t\t# word_features = [0.0] * len(self.__vocab_idxs)\n",
    "\t\t# The features representing each individual vocabulary word are at the beginning of the feature vector\n",
    "\t\t# word_features[self.__vocab_idxs[row[\"WORD\"]]] = 1.0\n",
    "\t\t# word_label = self.label_encoder.transform(row[\"WORD\"])\n",
    "\t\tword_label = row[\"WORD_LABEL\"]\n",
    "\t\t# print(\"Word label: {}\".format(word_label), file=sys.stderr)\n",
    "\t\t# \"OneHotEncoder.transform(..)\" returns a matrix even if only a single value is passed to it, so get just the first (and only) row\n",
    "\t\tword_features = self.onehot_encoder.transform(word_label)[0]\n",
    "\t\t# print(\"Word features: {}\".format(word_features), file=sys.stderr)\n",
    "\t\t# The word label for the one-hot encoding is that with the same index as the column that has a \"1\" value, i.e. the highest value in the vector of one-hot encoding values\n",
    "\t\t# inverse_label = np.argmax(word_features)\n",
    "\t\t# assert inverse_label == word_label\n",
    "\t\t# inverse_word = self.label_encoder.inverse_transform([inverse_label])\n",
    "\t\t# print(\"Inverse word label: {}\".format(inverse_label), file=sys.stderr)\n",
    "\t\tis_instructor = 1.0 if row[\"IS_INSTRUCTOR\"] else 0.0\n",
    "\t\t# is_target = 1.0 if row[\"IS_TARGET\"] else 0.0\n",
    "\t\tscore = row[\"PROBABILITY\"]\n",
    "\t\tother_features = np.array((is_instructor, score))\n",
    "\t\t# result = word_features + other_features\n",
    "\t\tresult = np.concatenate((word_features, other_features))\n",
    "\t\t# print(\"Created a vector of {} features.\".format(len(result)), file=sys.stderr)\n",
    "\t\treturn result\n",
    "\n",
    "\tdef __create_seq_feature_matrix(self, df: pd.DataFrame) -> np.matrix:\n",
    "\t\t# noinspection PyProtectedMember\n",
    "\t\treturn np.matrix(\n",
    "\t\t\ttuple(self.__create_datapoint_feature_array(row._asdict()) for row in df.itertuples(index=False)))\n",
    "\n",
    "\tdef __call__(self, df: pd.DataFrame) -> Iterator[np.matrix]:\n",
    "\t\tsequence_groups = df.groupby(\n",
    "\t\t\t(\"CROSS_VALIDATION_ITER\", \"DYAD\", \"UTT_START_TIME\", \"UTT_END_TIME\", \"ENTITY\"),\n",
    "\t\t\tas_index=False, sort=False)\n",
    "\t\treturn (self.__create_seq_feature_matrix(seq) for _, seq in sequence_groups)\n",
    "\n",
    "print(\"Splitting token sequences.\", file=sys.stderr)\n",
    "seq_matrix_factory = SequenceMatrixFactory(onehot_encoder)\n",
    "training_seqs = tuple(seq_matrix_factory(training_df))\n",
    "max_training_seq_len = max(m.shape[0] for m in training_seqs)\n",
    "print(\"Created a training dataset with a size of {} and a max sequence length of {}.\".format(len(training_seqs), max_training_seq_len), file=sys.stderr)\n",
    "test_seqs = tuple(seq_matrix_factory(test_df))\n",
    "max_test_seq_len = max(m.shape[0] for m in test_seqs)\n",
    "print(\"Created a test dataset with a size of {} and a max sequence length of {}.\".format(len(test_seqs), max_test_seq_len), file=sys.stderr)\n",
    "\n",
    "maxlen = max(max_training_seq_len, max_test_seq_len)\n",
    "print(\"Padding sequences to a length of {}.\".format(maxlen), file=sys.stderr)\n",
    "training_matrix = keras.preprocessing.sequence.pad_sequences(training_seqs, maxlen=maxlen, padding='pre', truncating='pre', value=0.)\n",
    "print(\"Batch training matrix shape: {}\".format(training_matrix.shape), file=sys.stderr)\n",
    "test_matrix = keras.preprocessing.sequence.pad_sequences(test_seqs, maxlen=maxlen, padding='pre', truncating='pre', value=0.)\n",
    "print(\"Batch test matrix shape: {}\".format(test_matrix.shape), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(291, 40, 250)\n",
      "(291, 40)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input shape: (40, 250)\n",
      "Units: 40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_23 (LSTM)               (None, 40)                46560     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 40)                1640      \n",
      "=================================================================\n",
      "Total params: 48,200\n",
      "Trainable params: 48,200\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Sequential\n",
    "\n",
    "training_x = training_matrix[:, :, :-1]\n",
    "print(\"Training X shape: {}\".format(training_x.shape), file=sys.stderr)\n",
    "assert len(training_x.shape) == 3\n",
    "training_y = training_matrix[:, :, -1]\n",
    "print(\"Training Y shape: {}\".format(training_y.shape), file=sys.stderr)\n",
    "assert len(training_y.shape) == 2\n",
    "\n",
    "model = Sequential()\n",
    "#word_embeddings = Embedding(len(vocab), embedding_vector_length, input_length=max_review_length)\n",
    "#model.add(word_embeddings)\n",
    "# model.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))\n",
    "# input shape is a pair of (timesteps, features) <https://stackoverflow.com/a/44583784/1391325>\n",
    "input_shape = training_x.shape[1:]\n",
    "print(\"Input shape: {}\".format(input_shape), file=sys.stderr)\n",
    "units = training_y.shape[1]\n",
    "print(\"Units: {}\".format(units), file=sys.stderr)\n",
    "lstm = LSTM(input_shape=input_shape, units=units)\n",
    "#lstm = LSTM(batch_input_shape = training_x.shape, stateful = True, units=len(training_y.shape))\n",
    "model.add(lstm)\n",
    "model.add(Dense(units, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "291/291 [==============================] - 1s 5ms/step - loss: 0.6882 - acc: 0.5668\n"
     ]
    }
   ],
   "source": [
    "# train LSTM\n",
    "\n",
    "#import keras.utils\n",
    "#import math\n",
    "\n",
    "#class CIFAR10Sequence(keras.utils.Sequence):\n",
    "\n",
    "#    def __init__(self, x_set, y_set, batch_size):\n",
    "#        self.x, self.y = x_set, y_set\n",
    "#        self.batch_size = batch_size\n",
    "\n",
    "#    def __len__(self):\n",
    "#        return math.ceil(len(self.x) / self.batch_size)\n",
    "\n",
    "#    def __getitem__(self, idx):\n",
    "#        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "#        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "#        return np.array(batch_x), np.array(batch_y)\n",
    "    \n",
    "#training_data_generator = CIFAR10Sequence(training_x, training_y, 32)\n",
    "\n",
    "#model.fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None)\n",
    "training_history = model.fit(training_x, training_y, epochs=1, verbose=1)\n",
    "#for seq_idx in range(0, len(training_x)):\n",
    "#   seq_x = training_x[seq_idx]\n",
    "#    print(seq_x.shape)\n",
    "#    seq_y = training_y[seq_idx]\n",
    "#    print(seq_y.shape)\n",
    "#    print(training_matrix.shape)\n",
    "#    training_x = np.array([training_matrix[:, :-1]])\n",
    "#print(training_x.shape)\n",
    "#    assert len(training_x.shape) == 3\n",
    "#    training_y = np.array([training_matrix[:, -1]])\n",
    "#print(training_y.shape)\n",
    "#    assert len(training_y.shape) == 3\n",
    "#    training_history = model.fit(x=training_x, y=training_y, epochs=1, verbose=1, batch_size=1)\n",
    "#    training_history = model.fit(x=seq_x, y=seq_y, epochs=1, verbose=1)\n",
    "#fit_generator(self, generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0)\n",
    "#training_history = model.fit_generator(training_data_generator, epochs=1, verbose=1)\n",
    "\n",
    "# train LSTM <https://machinelearningmastery.com/memory-in-a-long-short-term-memory-network/>\n",
    "#epochs = 5\n",
    "#for i in range(epochs):\n",
    "#    for seq_idx in range(0, training_x.shape[0]):\n",
    "        #x = training_x[i]\n",
    "#        x = training_x[0:31]\n",
    " #       x_oldshape = x.shape\n",
    " #       x_newshape = (-1, x_oldshape[0], x_oldshape[1])\n",
    "        #x = np.reshape(x, x_newshape)\n",
    "        #print(\"x.shape = {}\".format(x.shape), file=sys.stderr)\n",
    "        #y = training_y[i]\n",
    " #       y = training_y[0:31]\n",
    " #       y_oldshape = y.shape\n",
    " #       y_newshape = (-1, y_oldshape[0])\n",
    "        #y = np.reshape(y, y_newshape)\n",
    "        #print(\"y.shape = {}\".format(y.shape), file=sys.stderr)\n",
    " #       model.fit(x, y, epochs=1, batch_size=1, verbose=1, shuffle=False)\n",
    " #       model.reset_states()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test LSTM\n",
    "\n",
    "def onehot_encodings(features : np.array, onehot_encoder) -> np.array:\n",
    "    \"\"\"\n",
    "    :param features: A 2D array of feature vectors, each of which starts with one-hot encoding features. Any other features follow those.\n",
    "    :param onehot_encoder: The instance used to encode the original values.\n",
    "    :return: A 2D numpy array consisting only of one-hot encoding features.\n",
    "    \"\"\"\n",
    "    feature_count = onehot_encoder.n_values_[0]\n",
    "    return features[:,:feature_count + 1]\n",
    "\n",
    "\n",
    "test_x = test_matrix[:,:,:-1]\n",
    "print(test_x.shape)\n",
    "assert len(test_x.shape) == 3\n",
    "test_y = test_matrix[:,:,-1]\n",
    "print(test_y.shape)\n",
    "assert len(test_y.shape) == 2\n",
    "\n",
    "seq_predicted_values = model.predict(test_x, verbose=0)\n",
    "model.reset_states()\n",
    "#print(\"result.shape = {}\".format(seq_predicted_values.shape), file=sys.stderr)\n",
    "for i, tested_seq in enumerate(test_x):\n",
    "    word_features = onehot_encodings(tested_seq, onehot_encoder)\n",
    "    #print(\"word features: {}\".format(word_features))\n",
    "    #print(\"Tested sequence: {}\".format(tested_seq))\n",
    "    #print(\"Tested sequence shape: {}\".format(tested_seq.shape))\n",
    "    seq_labels = word_features.argmax(axis=1)\n",
    "    #print(\"Inverse labels: {}\".format(inverse_labels))\n",
    "    seq_words = label_encoder.inverse_transform(seq_labels)\n",
    "    print(\"Inverse word labels: {}\".format(seq_words))\n",
    "    predicted_values = seq_predicted_values[i]\n",
    "    assert tested_seq.shape[:-1] == predicted_values.shape\n",
    "    #assert tested_seq.shape[:-1] == actual_values.shape\n",
    "    #rint(\"Predicted values: {}\".format(predicted_values))\n",
    "    actual_values = test_y[i]\n",
    "    assert predicted_values.shape == actual_values.shape\n",
    "    \n",
    "    differences = predicted_values - actual_values\n",
    "    print(\"[Predicted_val - Actual_val]: {}\".format(differences))\n",
    "    #print(\"Actual values: {}\".format(actual_values))\n",
    "\t#print('X=%.1f y=%.1f, yhat=%.1f' % (seq1[i], seq1[i+1], result[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
