{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will read file \"D:\\Users\\tcshore\\Documents\\Projects\\Tangrams\\Data\\wordscores-inflected-small.tsv\".\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "import sys\n",
    "\n",
    "def get_project_root() -> str:\n",
    "    system = platform.system()\n",
    "    return r\"D:\\Users\\tcshore\\Documents\\Projects\\Tangrams\\Data\" if system == \"Windows\" else \"/home/tshore/Projects/tangrams-restricted/Data\"\n",
    "\n",
    "infile_path = os.path.join(get_project_root(), \"wordscores-inflected-small.tsv\")\n",
    "print(\"Will read file \\\"{}\\\".\".format(infile_path), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading \"D:\\Users\\tcshore\\Documents\\Projects\\Tangrams\\Data\\wordscores-inflected-small.tsv\" using encoding \"windows-1252\".\n",
      "Read 67060 cross-validation results for 3 dyad(s).\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "RESULTS_FILE_CSV_DIALECT = csv.excel_tab\n",
    "# NOTE: \"category\" dtype doesn't work with pandas-0.21.0 but does with pandas-0.21.1\n",
    "__RESULTS_FILE_DTYPES = {\"DYAD\": \"category\", \"ENTITY\" : \"category\", \"IS_TARGET\": bool, \"IS_OOV\": bool,\n",
    "\t\t\t\t \"IS_INSTRUCTOR\": bool, \"SHAPE\": \"category\", \"ONLY_INSTRUCTOR\": bool, \"WEIGHT_BY_FREQ\": bool}\n",
    "\n",
    "def read_results_file(inpath: str, encoding: str) -> pd.DataFrame:\n",
    "\tprint(\"Reading \\\"{}\\\" using encoding \\\"{}\\\".\".format(inpath, encoding), file=sys.stderr)\n",
    "\tresult = pd.read_csv(inpath, dialect=RESULTS_FILE_CSV_DIALECT, sep=RESULTS_FILE_CSV_DIALECT.delimiter,\n",
    "\t\t\t\t\t\t float_precision=\"round_trip\",\n",
    "\t\t\t\t\t\t encoding=encoding, memory_map=True, dtype=__RESULTS_FILE_DTYPES)\n",
    "\treturn result\n",
    "\n",
    "cv_results = read_results_file(infile_path, \"windows-1252\")\n",
    "print(\"Read {} cross-validation results for {} dyad(s).\".format(cv_results.shape[0], cv_results[\"DYAD\"].nunique()),\n",
    "      file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Replacing OOV words with label \"__OOV__\".\n"
     ]
    }
   ],
   "source": [
    "OOV_LABEL = \"__OOV__\"\n",
    "    \n",
    "print(\"Replacing OOV words with label \\\"{}\\\".\".format(OOV_LABEL), file=sys.stderr)\n",
    "cv_results.loc[cv_results[\"IS_OOV\"] == True, \"WORD\"] = OOV_LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting one-hot encoder for vocabulary of size 249.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Create vocab before splitting training and testing DFs so that the word feature set is stable\n",
    "print(\"Fitting one-hot encoder for vocabulary of size {}.\".format(cv_results[\"WORD\"].nunique()), file=sys.stderr)\n",
    "# https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "vocab_labels = label_encoder.fit_transform(cv_results[\"WORD\"])\n",
    "cv_results[\"WORD_LABEL\"] = vocab_labels\n",
    "#print(vocab_labels)\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "vocab_labels = vocab_labels.reshape(len(vocab_labels), 1)\n",
    "onehot_encoder.fit(vocab_labels)\n",
    "#assert onehot_encoder.n_values_ == len(vocab_words)\n",
    "#vocab_onehot_encoded = onehot_encoder.fit_transform(vocab_labels)\n",
    "#print(vocab_onehot_encoded)\n",
    "# invert first example\n",
    "#inverted = label_encoder.inverse_transform([np.argmax(vocab_onehot_encoded[0, :])])\n",
    "#print(inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training set dyads: ['20', '21']\n",
      "Test set dyads: ['22']\n",
      "Found 42978 non-target rows and 2262 target rows. Ratio: 19.0\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def find_target_ref_rows(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\tresult = df.loc[df[\"IS_TARGET\"] == True]\n",
    "\tresult_row_count = result.shape[0]\n",
    "\tcomplement_row_count = df.loc[~df.index.isin(result.index)].shape[0]\n",
    "\tassert result_row_count + complement_row_count == df.shape[0]\n",
    "\tprint(\"Found {} non-target rows and {} target rows. Ratio: {}\".format(complement_row_count, result_row_count,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t complement_row_count / float(\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t result_row_count)), file=sys.stderr)\n",
    "\treturn result\n",
    "\n",
    "def split_training_testing(df: pd.DataFrame, test_set_size: int) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "\tdyad_ids = df[\"DYAD\"].unique()\n",
    "\ttraining_set_size = len(dyad_ids) - test_set_size\n",
    "\tif training_set_size < 1:\n",
    "\t\traise ValueError(\"Desired test set size is {} but only {} dyads found.\".format(test_set_size, len(dyad_ids)))\n",
    "\telse:\n",
    "\t\ttraining_set_dyads = frozenset(np.random.choice(dyad_ids, training_set_size, replace=False))\n",
    "\t\tassert len(training_set_dyads) == training_set_size\n",
    "\t\tprint(\"Training set dyads: {}\".format(sorted(training_set_dyads)), file=sys.stderr)\n",
    "\t\ttraining_set_idxs = df[\"DYAD\"].isin(training_set_dyads)\n",
    "\t\ttraining_set = df.loc[training_set_idxs]\n",
    "\t\ttest_set = df.loc[~training_set_idxs]\n",
    "\t\ttest_set_dyads = frozenset(test_set[\"DYAD\"].unique())\n",
    "\t\tprint(\"Test set dyads: {}\".format(sorted(test_set_dyads)), file=sys.stderr)\n",
    "\t\tassert not frozenset(training_set[\"DYAD\"].unique()).intersection(frozenset(test_set_dyads))\n",
    "\t\treturn training_set, test_set\n",
    "    \n",
    "# https://stackoverflow.com/a/47815400/1391325\n",
    "cv_results.sort_values(\"TOKEN_SEQ_ORDINALITY\", inplace=True)\n",
    "training_df, test_df = split_training_testing(cv_results, 1)\n",
    "# Only train on \"true\" referents\n",
    "training_df = find_target_ref_rows(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from typing import DefaultDict, Iterator, List, Sequence\n",
    "\n",
    "import keras.utils\n",
    "\n",
    "class DataGeneratorFactory(object):\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef __group_by_seq_len(seq_xy: pd.Series) -> DefaultDict[int, List[Tuple[np.matrix, np.ndarray]]]:\n",
    "\t\tresult = defaultdict(list)\n",
    "\t\tfor xy in seq_xy:\n",
    "\t\t\tseq_len = xy[0].shape[0]\n",
    "\t\t\tresult[seq_len].append(xy)\n",
    "\t\treturn result\n",
    "\n",
    "\tdef __init__(self, onehot_encoder: OneHotEncoder):\n",
    "\t\tself.onehot_encoder = onehot_encoder\n",
    "\n",
    "\t@property\n",
    "\tdef input_feature_count(self) -> int:\n",
    "\t\tword_features = self.onehot_encoder.n_values_[0]\n",
    "\t\treturn word_features + 1\n",
    "\n",
    "\t@property\n",
    "\tdef output_feature_count(self) -> int:\n",
    "\t\treturn 1\n",
    "\n",
    "\tdef __call__(self, df: pd.DataFrame) -> \"TokenSequenceSequence\":\n",
    "\t\tsequence_groups = df.groupby(\n",
    "\t\t\t(\"CROSS_VALIDATION_ITER\", \"DYAD\", \"ROUND\", \"UTT_START_TIME\", \"UTT_END_TIME\", \"ENTITY\"), sort=False)\n",
    "\t\tprint(\"Generating data for {} entity token sequence(s).\".format(len(sequence_groups)), file=sys.stderr)\n",
    "\t\tseq_xy = sequence_groups.apply(self.__create_seq_xy)\n",
    "\t\tlen_dict = self.__group_by_seq_len(seq_xy)\n",
    "\t\tprint(\"Created {} batches, one for each unique sequence length.\".format(len(len_dict)), file=sys.stderr)\n",
    "\t\tseq_batches_by_len = tuple(len_dict.values())\n",
    "\t\treturn TokenSequenceSequence(seq_batches_by_len)\n",
    "\n",
    "\tdef __create_datapoint_x(self, row: pd.Series) -> Tuple[np.ndarray,]:\n",
    "\t\t# word_features = [0.0] * len(self.__vocab_idxs)\n",
    "\t\t# The features representing each individual vocabulary word are at the beginning of the feature vector\n",
    "\t\t# word_features[self.__vocab_idxs[row[\"WORD\"]]] = 1.0\n",
    "\t\t# word_label = self.label_encoder.transform(row[\"WORD\"])\n",
    "\t\tword_label = row[\"WORD_LABEL\"]\n",
    "\t\t# print(\"Word label: {}\".format(word_label), file=sys.stderr)\n",
    "\t\t# \"OneHotEncoder.transform(..)\" returns a matrix even if only a single value is passed to it, so get just the first (and only) row\n",
    "\t\tword_features = self.onehot_encoder.transform(word_label)[0]\n",
    "\t\t# print(\"Word features: {}\".format(word_features), file=sys.stderr)\n",
    "\t\t# The word label for the one-hot encoding is that with the same index as the column that has a \"1\" value, i.e. the highest value in the vector of one-hot encoding values\n",
    "\t\t# inverse_label = np.argmax(word_features)\n",
    "\t\t# assert inverse_label == word_label\n",
    "\t\t# inverse_word = self.label_encoder.inverse_transform([inverse_label])\n",
    "\t\t# print(\"Inverse word label: {}\".format(inverse_label), file=sys.stderr)\n",
    "\t\tis_instructor = 1.0 if row[\"IS_INSTRUCTOR\"] else 0.0\n",
    "\t\t# is_target = 1.0 if row[\"IS_TARGET\"] else 0.0\n",
    "\t\tother_features = np.array((is_instructor,))\n",
    "\t\t# result = word_features + other_features\n",
    "\t\tresult = np.concatenate((word_features, other_features))\n",
    "\t\t# print(\"Created a vector of {} features.\".format(len(result)), file=sys.stderr)\n",
    "\t\t# NOTE: Returning a tuple is a hack in order to return an instance of \"np.ndarray\" from \"DataFrame.apply()\"\n",
    "\t\treturn result,\n",
    "\n",
    "\tdef __create_seq_x_matrix(self, seq_df: pd.DataFrame) -> np.matrix:\n",
    "\t\t# NOTE: The returned tuples have to be unpacked outside of the \"apply(..)\" function\n",
    "\t\tvectors = seq_df.apply(self.__create_datapoint_x, axis=1)\n",
    "\t\treturn np.matrix(tuple(vector[0] for vector in vectors))\n",
    "\n",
    "\tdef __create_seq_xy(self, seq_df: pd.DataFrame) -> Tuple[np.matrix, np.ndarray]:\n",
    "\t\tx = self.__create_seq_x_matrix(seq_df)\n",
    "\t\ty = seq_df[\"PROBABILITY\"].values\n",
    "\t\treturn x, y\n",
    "\n",
    "\n",
    "class TokenSequenceSequence(keras.utils.Sequence):\n",
    "\t\"\"\"\n",
    "\tA sequence (i.e. less confusingly a dataset) of token sequences, each of which is for a given distinct entity, i.e. possible referent.\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, seq_batches_by_len: Sequence[Sequence[Tuple[np.matrix, np.ndarray]]]):\n",
    "\t\tself.seq_batches_by_len = seq_batches_by_len\n",
    "\n",
    "\tdef __len__(self) -> int:\n",
    "\t\treturn len(self.seq_batches_by_len)\n",
    "\n",
    "\tdef __getitem__(self, idx: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "\t\t# print(\"Getting batch idx {}.\".format(idx), file=sys.stderr)\n",
    "\t\tbatch = self.seq_batches_by_len[idx]\n",
    "\t\tseq_x = tuple(x for x, y in batch)\n",
    "\t\tx = np.asarray(seq_x)\n",
    "\t\t# print(\"X shape: {}\".format(x.shape), file=sys.stderr)\n",
    "\t\tseq_y = tuple(y for x, y in batch)\n",
    "\t\tif any(len(y.shape) > 1 for y in seq_y):\n",
    "\t\t\traise ValueError(\"Output feature vectors with a dimensionality greater than 1 are not supported.\")\n",
    "\t\ty = np.asarray(tuple(y[0] for y in seq_y))\n",
    "\t\t# y = np.asarray(seq_y)\n",
    "\t\t# print(\"Y shape: {}\".format(y.shape), file=sys.stderr)\n",
    "\t\treturn x, y\n",
    "    \n",
    "\n",
    "data_generator_factory = DataGeneratorFactory(onehot_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input shape: (None, 250)\n",
      "Units: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 1)                 1008      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 1,010\n",
      "Trainable params: 1,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Sequential\n",
    "\n",
    "def create_model(input_feature_count: int, output_feature_count: int) -> Sequential:\n",
    "\tresult = Sequential()\n",
    "\t# word_embeddings = Embedding(len(vocab), embedding_vector_length, input_length=max_review_length)\n",
    "\t# model.add(word_embeddings)\n",
    "\t# model.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))\n",
    "\t# input shape is a pair of (timesteps, features) <https://stackoverflow.com/a/44583784/1391325>\n",
    "\tinput_shape = (None, input_feature_count)\n",
    "\tprint(\"Input shape: {}\".format(input_shape), file=sys.stderr)\n",
    "\tunits = output_feature_count\n",
    "\tprint(\"Units: {}\".format(units), file=sys.stderr)\n",
    "\tlstm = LSTM(input_shape=input_shape, units=units, dropout=0.1, recurrent_dropout=0.1)\n",
    "\t# lstm = LSTM(batch_input_shape = training_x.shape, stateful = True, units=len(training_y.shape))\n",
    "\tresult.add(lstm)\n",
    "\tresult.add(Dense(units, activation='softmax'))\n",
    "\tresult.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "\tresult.summary(print_fn=lambda line : print(line, file=sys.stderr))\n",
    "\treturn result\n",
    "\n",
    "model = create_model(data_generator_factory.input_feature_count, data_generator_factory.output_feature_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating training data token sequences.\n",
      "Generating data for 273 entity token sequence(s).\n",
      "Created 28 batches, one for each unique sequence length.\n",
      "Generating validation data token sequences.\n",
      "Found 20729 non-target rows and 1091 target rows. Ratio: 19.0\n",
      "Generating data for 143 entity token sequence(s).\n",
      "Created 28 batches, one for each unique sequence length.\n",
      "Input shape: (None, 250)\n",
      "Units: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 1)                 1008      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 1,010\n",
      "Trainable params: 1,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model using 250 epoch(s).\n",
      "Using 4 worker thread(s).\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "print(\"Generating training data token sequences.\", file=sys.stderr)\n",
    "training_data_generator = data_generator_factory(training_df)\n",
    "print(\"Generating validation data token sequences.\", file=sys.stderr)\n",
    "validation_data_generator = data_generator_factory(find_target_ref_rows(test_df))\n",
    "\n",
    "# train LSTM\n",
    "model = create_model(data_generator_factory.input_feature_count, data_generator_factory.output_feature_count)\n",
    "# train LSTM\n",
    "epochs = 250\n",
    "print(\"Training model using {} epoch(s).\".format(epochs), file=sys.stderr)\n",
    "workers = max(multiprocessing.cpu_count() // 2, 1)\n",
    "print(\"Using {} worker thread(s).\".format(workers), file=sys.stderr)\n",
    "training_history = model.fit_generator(training_data_generator, epochs=epochs, verbose=0,\n",
    "                                       validation_data=validation_data_generator, use_multiprocessing=False,\n",
    "                                       workers=workers)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d1760f5e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d17612af98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "\n",
    "# list all data in history\n",
    "#print(training_history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(training_history.history['acc'])\n",
    "plt.plot(training_history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(training_history.history['loss'])\n",
    "plt.plot(training_history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will test 143 token sequences.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'seq_matrix_factory' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-e6dfb30642d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     92\u001b[0m                 as_index=False, sort=False)\n\u001b[0;32m     93\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Will test {} token sequences.\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_seq_groups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m \u001b[0mtest_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoken_seq_groups\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtest_token_seq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_matrix_factory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_encoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0monehot_encoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    803\u001b[0m         \u001b[1;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    804\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0moption_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mode.chained_assignment'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 805\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_apply_general\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    806\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    807\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_python_apply_general\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby.py\u001b[0m in \u001b[0;36m_python_apply_general\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    807\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_python_apply_general\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    808\u001b[0m         keys, values, mutated = self.grouper.apply(f, self._selected_obj,\n\u001b[1;32m--> 809\u001b[1;33m                                                    self.axis)\n\u001b[0m\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    811\u001b[0m         return self._wrap_applied_output(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, f, data, axis)\u001b[0m\n\u001b[0;32m   1967\u001b[0m             \u001b[1;31m# group might be modified\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1968\u001b[0m             \u001b[0mgroup_axes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_axes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1969\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1970\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_is_indexed_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup_axes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1971\u001b[0m                 \u001b[0mmutated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-e6dfb30642d8>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(group)\u001b[0m\n\u001b[0;32m     92\u001b[0m                 as_index=False, sort=False)\n\u001b[0;32m     93\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Will test {} token sequences.\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_seq_groups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m \u001b[0mtest_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoken_seq_groups\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtest_token_seq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_matrix_factory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_encoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0monehot_encoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'seq_matrix_factory' is not defined"
     ]
    }
   ],
   "source": [
    "# test LSTM\n",
    "\n",
    "import logging\n",
    "\n",
    "def onehot_encodings(features : np.ndarray, onehot_encoder) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    :param features: A 2D array of feature vectors, each of which starts with one-hot encoding features. Any other features follow those.\n",
    "    :param onehot_encoder: The instance used to encode the original values.\n",
    "    :return: A 2D numpy array consisting only of one-hot encoding features.\n",
    "    \"\"\"\n",
    "    feature_count = onehot_encoder.n_values_[0]\n",
    "    return features[:,:feature_count + 1]\n",
    "\n",
    "def onehot_encoded_word(onehot_encodings : np.ndarray, label_encoder) -> str:\n",
    "    # Check if there are any non-zero values\n",
    "    if onehot_encodings.any():\n",
    "        word_label = onehot_encodings.argmax()\n",
    "        result = label_encoder.inverse_transform([word_label])[0]\n",
    "    else:\n",
    "        result = \"(padding)\"\n",
    "    return result\n",
    "\n",
    "def word(features : np.ndarray, label_encoder, onehot_encoder) -> str:\n",
    "    feature_count = onehot_encoder.n_values_[0]\n",
    "    onehot = features[:feature_count + 1]\n",
    "    return onehot_encoded_word(onehot, label_encoder)\n",
    "\n",
    "def word_seq(x : np.ndarray, label_encoder, onehot_encoder) -> np.ndarray:\n",
    "    word_features = onehot_encodings(x, onehot_encoder)\n",
    "    return np.apply_along_axis(lambda arr : onehot_encoded_word(arr, label_encoder), 1, word_features)\n",
    "\n",
    "def create_padded_seq_matrix(group: pd.DataFrame, seq_matrix_factory):\n",
    "    seq_matrices = tuple(seq_matrix_factory(group))\n",
    "    print(\"Created a dataset composed of {} sequence(s), with a max sequence length of {}.\".format(len(seq_matrices), max(m.shape[0] for m in seq_matrices)), file=sys.stderr)\n",
    "    print(\"Padding data sequences.\", file=sys.stderr)\n",
    "    return keras.preprocessing.sequence.pad_sequences(seq_matrices, maxlen=None, padding='pre', truncating='pre', value=0.0)\n",
    "    \n",
    "\n",
    "def test_token_seq(token_seq_group : pd.DataFrame, seq_matrix_factory, model: Sequential, label_encoder, onehot_encoder): \n",
    "    entity_groups = token_seq_group.groupby(\"ENTITY\", as_index=False, sort=False)\n",
    "    entity_matrices = tuple(entity_groups.apply(lambda group : (group.name, seq_matrix_factory(group))))\n",
    "    #for _, m in entity_matrices:\n",
    "    #    y = m[:, -1]\n",
    "    #    print(y) \n",
    "    \n",
    "    logging.debug(\"Created matrices for %d entities, with a max sequence length of %d.\", len(entity_matrices), max(m.shape[0] for _, m in entity_matrices))\n",
    "    logging.debug(\"Padding entity test data sequences.\")\n",
    "    test_batch = keras.preprocessing.sequence.pad_sequences(tuple(m for _, m in entity_matrices), maxlen=None, padding='pre', value=0.0, dtype='int32')\n",
    "    print(test_batch[:, :, :-1])\n",
    "    logging.debug(\"Test batch shape: %s\", test_batch.shape)\n",
    "    test_x, test_y = split_xy(test_batch)\n",
    "    \n",
    "    \n",
    "    for group_name, m in entity_matrices:\n",
    "        m = np.expand_dims(m, axis=0)\n",
    "        print(m.shape)\n",
    "        x = m[:, :, :-1]\n",
    "        print(x.shape)\n",
    "        y = m[:, :, -1]\n",
    "        print(y.shape)\n",
    "        model.predict(x, verbose=0, batch_size=1)\n",
    "    \n",
    "    #logging.debug(\"Test X shape: %s\", test_x.shape)\n",
    "    #logging.debug(\"Test Y shape: %s\", test_y.shape)\n",
    "    # Classify all sets of datapoints for each entity as a single batch\n",
    "    #entity_predicted_scores = model.predict(test_x, verbose=0)\n",
    "    \n",
    "    max_score = 0\n",
    "    for idx in range(0, test_x.shape[0]):\n",
    "        entity_id = entity_matrices[idx][0]\n",
    "        # Get the scores for each entity\n",
    "        entity_x = test_x[idx]\n",
    "        logging.debug(\"Re-scoring token sequence for entity %s: %s\", entity_id, word_seq(entity_x, label_encoder, onehot_encoder))\n",
    "        entity_orig_scores = test_y[idx]\n",
    "        for tok_idx, features in enumerate(entity_x):\n",
    "            w = word(features, label_encoder, onehot_encoder)\n",
    "            s = entity_orig_scores[tok_idx]\n",
    "            print(\"{}; Orig score: {}\".format(w, s))\n",
    "        \n",
    "        max_score = max(max_score, entity_orig_scores.argmax())\n",
    "        #print(entity_orig_scores)\n",
    "        orig_score = entity_orig_scores.sum()\n",
    "        #predicted_scores = entity_predicted_scores[idx]\n",
    "        #print(predicted_scores)\n",
    "        #rescored = np.dot(entity_orig_scores, predicted_scores)\n",
    "        #logging.warning(\"Original score: %f; New score: %f\", orig_score, rescored)\n",
    "                        \n",
    "    print(\"Max score: {}\".format(max_score))\n",
    "            \n",
    "token_seq_groups = test_df.groupby(\n",
    "                (\"CROSS_VALIDATION_ITER\", \"DYAD\", \"ROUND\", \"UTT_START_TIME\", \"UTT_END_TIME\"),\n",
    "                as_index=False, sort=False)\n",
    "print(\"Will test {} token sequences.\".format(len(token_seq_groups)), file=sys.stderr)\n",
    "test_results = token_seq_groups.apply(lambda group: test_token_seq(group, seq_matrix_factory, model, label_encoder, onehot_encoder))\n",
    "\n",
    "                        \n",
    "#test_x, test_y = split_xy(test_matrix)\n",
    "#print(\"Training X shape: {}\".format(test_x.shape), file=sys.stderr)\n",
    "#print(\"Training Y shape: {}\".format(test_y.shape), file=sys.stderr)\n",
    "\n",
    "#seq_predicted_values = model.predict(test_x, verbose=0)\n",
    "#model.reset_states()\n",
    "#print(\"result.shape = {}\".format(seq_predicted_values.shape), file=sys.stderr)\n",
    "#for i, tested_seq in enumerate(test_x):\n",
    "#    word_features = onehot_encodings(tested_seq, onehot_encoder)\n",
    "    #print(\"word features: {}\".format(word_features))\n",
    "    #print(\"Tested sequence: {}\".format(tested_seq))\n",
    "    #print(\"Tested sequence shape: {}\".format(tested_seq.shape))\n",
    "    #seq_labels = word_features.argmax(axis=1)\n",
    "    #print(\"Inverse labels: {}\".format(inverse_labels))\n",
    "    #seq_words = label_encoder.inverse_transform(seq_labels)\n",
    "#    seq_words = np.apply_along_axis(lambda arr : onehot_encoded_word(arr, label_encoder), 1, word_features)\n",
    "    #print(\"Inverse word labels: {}\".format(seq_words))\n",
    "#    predicted_values = seq_predicted_values[i]\n",
    "#    assert tested_seq.shape[:-1] == predicted_values.shape\n",
    "    #assert tested_seq.shape[:-1] == actual_values.shape\n",
    "    #rint(\"Predicted values: {}\".format(predicted_values))\n",
    "#    actual_values = test_y[i]\n",
    "#    assert predicted_values.shape == actual_values.shape\n",
    "    \n",
    "#    differences = predicted_values - actual_values\n",
    "#    print(\"[Predicted_val - Actual_val]: {}\".format(differences))\n",
    "    #print(\"Actual values: {}\".format(actual_values))\n",
    "\t#print('X=%.1f y=%.1f, yhat=%.1f' % (seq1[i], seq1[i+1], result[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
