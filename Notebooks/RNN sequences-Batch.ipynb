{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will read file \"D:\\Users\\tcshore\\Documents\\Projects\\Tangrams\\Data\\wordscores-inflected-small.tsv\".\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "import sys\n",
    "\n",
    "def get_project_root() -> str:\n",
    "    system = platform.system()\n",
    "    return r\"D:\\Users\\tcshore\\Documents\\Projects\\Tangrams\\Data\" if system == \"Windows\" else \"/home/tshore/Projects/tangrams-restricted/Data\"\n",
    "\n",
    "infile_path = os.path.join(get_project_root(), \"wordscores-inflected-small.tsv\")\n",
    "print(\"Will read file \\\"{}\\\".\".format(infile_path), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading \"D:\\Users\\tcshore\\Documents\\Projects\\Tangrams\\Data\\wordscores-inflected-small.tsv\" using encoding \"windows-1252\".\n",
      "Read 67060 cross-validation results for 3 dyad(s).\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "RESULTS_FILE_CSV_DIALECT = csv.excel_tab\n",
    "# NOTE: \"category\" dtype doesn't work with pandas-0.21.0 but does with pandas-0.21.1\n",
    "__RESULTS_FILE_DTYPES = {\"DYAD\": \"category\", \"ENTITY\" : \"category\", \"IS_TARGET\": bool, \"IS_OOV\": bool,\n",
    "\t\t\t\t \"IS_INSTRUCTOR\": bool, \"SHAPE\": \"category\", \"ONLY_INSTRUCTOR\": bool, \"WEIGHT_BY_FREQ\": bool}\n",
    "\n",
    "def read_results_file(inpath: str, encoding: str) -> pd.DataFrame:\n",
    "\tprint(\"Reading \\\"{}\\\" using encoding \\\"{}\\\".\".format(inpath, encoding), file=sys.stderr)\n",
    "\tresult = pd.read_csv(inpath, dialect=RESULTS_FILE_CSV_DIALECT, sep=RESULTS_FILE_CSV_DIALECT.delimiter,\n",
    "\t\t\t\t\t\t float_precision=\"round_trip\",\n",
    "\t\t\t\t\t\t encoding=encoding, memory_map=True, dtype=__RESULTS_FILE_DTYPES)\n",
    "\treturn result\n",
    "\n",
    "cv_results = read_results_file(infile_path, \"windows-1252\")\n",
    "print(\"Read {} cross-validation results for {} dyad(s).\".format(cv_results.shape[0], cv_results[\"DYAD\"].nunique()),\n",
    "      file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Replacing OOV words with label \"__OOV__\".\n"
     ]
    }
   ],
   "source": [
    "OOV_LABEL = \"__OOV__\"\n",
    "    \n",
    "print(\"Replacing OOV words with label \\\"{}\\\".\".format(OOV_LABEL), file=sys.stderr)\n",
    "cv_results.loc[cv_results[\"IS_OOV\"] == True, \"WORD\"] = OOV_LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting one-hot encoder for vocabulary of size 249.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Create vocab before splitting training and testing DFs so that the word feature set is stable\n",
    "print(\"Fitting one-hot encoder for vocabulary of size {}.\".format(cv_results[\"WORD\"].nunique()), file=sys.stderr)\n",
    "# https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "vocab_labels = label_encoder.fit_transform(cv_results[\"WORD\"])\n",
    "cv_results[\"WORD_LABEL\"] = vocab_labels\n",
    "#print(vocab_labels)\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "vocab_labels = vocab_labels.reshape(len(vocab_labels), 1)\n",
    "onehot_encoder.fit(vocab_labels)\n",
    "#assert onehot_encoder.n_values_ == len(vocab_words)\n",
    "#vocab_onehot_encoded = onehot_encoder.fit_transform(vocab_labels)\n",
    "#print(vocab_onehot_encoded)\n",
    "# invert first example\n",
    "#inverted = label_encoder.inverse_transform([np.argmax(vocab_onehot_encoded[0, :])])\n",
    "#print(inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training set dyads: ['21', '22']\n",
      "Test set dyads: ['20']\n",
      "Found 43947 non-target rows and 2313 target rows. Ratio: 19.0\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def find_target_ref_rows(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\tresult = df.loc[df[\"IS_TARGET\"] == True]\n",
    "\tresult_row_count = result.shape[0]\n",
    "\tcomplement_row_count = df.loc[~df.index.isin(result.index)].shape[0]\n",
    "\tassert result_row_count + complement_row_count == df.shape[0]\n",
    "\tprint(\"Found {} non-target rows and {} target rows. Ratio: {}\".format(complement_row_count, result_row_count,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t complement_row_count / float(\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t result_row_count)), file=sys.stderr)\n",
    "\treturn result\n",
    "\n",
    "def split_training_testing(df: pd.DataFrame, test_set_size: int) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "\tdyad_ids = df[\"DYAD\"].unique()\n",
    "\ttraining_set_size = len(dyad_ids) - test_set_size\n",
    "\tif training_set_size < 1:\n",
    "\t\traise ValueError(\"Desired test set size is {} but only {} dyads found.\".format(test_set_size, len(dyad_ids)))\n",
    "\telse:\n",
    "\t\ttraining_set_dyads = frozenset(np.random.choice(dyad_ids, training_set_size, replace=False))\n",
    "\t\tassert len(training_set_dyads) == training_set_size\n",
    "\t\tprint(\"Training set dyads: {}\".format(sorted(training_set_dyads)), file=sys.stderr)\n",
    "\t\ttraining_set_idxs = df[\"DYAD\"].isin(training_set_dyads)\n",
    "\t\ttraining_set = df.loc[training_set_idxs]\n",
    "\t\ttest_set = df.loc[~training_set_idxs]\n",
    "\t\ttest_set_dyads = frozenset(test_set[\"DYAD\"].unique())\n",
    "\t\tprint(\"Test set dyads: {}\".format(sorted(test_set_dyads)), file=sys.stderr)\n",
    "\t\tassert not frozenset(training_set[\"DYAD\"].unique()).intersection(frozenset(test_set_dyads))\n",
    "\t\treturn training_set, test_set\n",
    "    \n",
    "# https://stackoverflow.com/a/47815400/1391325\n",
    "cv_results.sort_values(\"TOKEN_SEQ_ORDINALITY\", inplace=True)\n",
    "training_df, test_df = split_training_testing(cv_results, 1)\n",
    "# Only train on \"true\" referents\n",
    "training_df = find_target_ref_rows(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Maximum sequence length in dataset is 40.\n",
      "Splitting training data token sequences.\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterator, List, Mapping\n",
    "\n",
    "import keras.preprocessing.sequence\n",
    "\n",
    "class SequenceMatrixFactory(object):\n",
    "    \n",
    "\tdef __init__(self, onehot_encoder: OneHotEncoder):\n",
    "\t\tself.onehot_encoder = onehot_encoder\n",
    "\n",
    "\t@property\n",
    "\tdef feature_count(self) -> int:\n",
    "\t\tword_features = self.onehot_encoder.n_values_[0]\n",
    "\t\treturn word_features + 2\n",
    "\n",
    "\tdef __create_datapoint_feature_array(self, row: pd.Series) -> Tuple[np.ndarray]:\n",
    "\t\t# word_features = [0.0] * len(self.__vocab_idxs)\n",
    "\t\t# The features representing each individual vocabulary word are at the beginning of the feature vector\n",
    "\t\t# word_features[self.__vocab_idxs[row[\"WORD\"]]] = 1.0\n",
    "\t\t# word_label = self.label_encoder.transform(row[\"WORD\"])\n",
    "\t\tword_label = row[\"WORD_LABEL\"]\n",
    "\t\t# print(\"Word label: {}\".format(word_label), file=sys.stderr)\n",
    "\t\t# \"OneHotEncoder.transform(..)\" returns a matrix even if only a single value is passed to it, so get just the first (and only) row\n",
    "\t\tword_features = self.onehot_encoder.transform(word_label)[0]\n",
    "\t\t# print(\"Word features: {}\".format(word_features), file=sys.stderr)\n",
    "\t\t# The word label for the one-hot encoding is that with the same index as the column that has a \"1\" value, i.e. the highest value in the vector of one-hot encoding values\n",
    "\t\t# inverse_label = np.argmax(word_features)\n",
    "\t\t# assert inverse_label == word_label\n",
    "\t\t# inverse_word = self.label_encoder.inverse_transform([inverse_label])\n",
    "\t\t# print(\"Inverse word label: {}\".format(inverse_label), file=sys.stderr)\n",
    "\t\tis_instructor = 1.0 if row[\"IS_INSTRUCTOR\"] else 0.0\n",
    "\t\t# is_target = 1.0 if row[\"IS_TARGET\"] else 0.0\n",
    "\t\tscore = row[\"PROBABILITY\"]\n",
    "\t\t#print(score)\n",
    "\t\tother_features = np.array((is_instructor, score))\n",
    "\t\t# result = word_features + other_features\n",
    "\t\tresult = np.concatenate((word_features, other_features))\n",
    "\t\t# print(\"Created a vector of {} features.\".format(len(result)), file=sys.stderr)\n",
    "\t\t# NOTE: Returning a tuple is a hack in order to return an instance of \"np.ndarray\" from \"DataFrame.apply()\"        \n",
    "\t\treturn result,\n",
    "\n",
    "\tdef __call__(self, df: pd.DataFrame) -> np.matrix:\n",
    "\t\t# NOTE: The returned tuples have to be unpacked outside of the \"apply(..)\" function\n",
    "\t\tvectors = df.apply(self.__create_datapoint_feature_array, axis=1)        \n",
    "\t\treturn np.matrix(tuple(vector[0] for vector in vectors))\n",
    "    \n",
    "def create_training_batch_array(df: pd.DataFrame, seq_matrix_factory) -> np.ndarray:\n",
    "\tsequence_groups = df.groupby(\n",
    "\t\t\t(\"CROSS_VALIDATION_ITER\", \"DYAD\", \"ROUND\", \"UTT_START_TIME\", \"UTT_END_TIME\", \"ENTITY\"),\n",
    "\t\t\tas_index=False, sort=False)\n",
    "\tseq_matrices = tuple(sequence_groups.apply(seq_matrix_factory))\n",
    "\tprint(\"Created a training dataset composed of {} sequence(s), with a max sequence length of {}.\".format(len(seq_matrices), max(m.shape[0] for m in seq_matrices)), file=sys.stderr)\n",
    "\t#print(\"Padding training data sequences to a length of {}.\".format(max_seq_len), file=sys.stderr)\n",
    "\treturn keras.preprocessing.sequence.pad_sequences(seq_matrices, maxlen=None, padding='pre', truncating='pre', value=0.)\n",
    "    \n",
    "max_seq_len = cv_results[\"TOKEN_SEQ_ORDINALITY\"].max()\n",
    "print(\"Maximum sequence length in dataset is {}.\".format(max_seq_len), file=sys.stderr)\n",
    "\n",
    "print(\"Splitting training data token sequences.\", file=sys.stderr)\n",
    "seq_matrix_factory = SequenceMatrixFactory(onehot_encoder)\n",
    "training_batch = create_training_batch_array(training_df, seq_matrix_factory)\n",
    "print(\"Batch training array shape: {}\".format(training_batch.shape), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Sequential\n",
    "\n",
    "def split_xy(matrix : np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    x = matrix[:, :, :-1]\n",
    "    assert len(x.shape) == 3\n",
    "    y = matrix[:, :, -1]\n",
    "    assert len(y.shape) == 2\n",
    "    return x, y\n",
    "\n",
    "def create_model(training_x: np.ndarray, training_y: np.ndarray) -> Sequential:\n",
    "\tresult = Sequential()\n",
    "\t# word_embeddings = Embedding(len(vocab), embedding_vector_length, input_length=max_review_length)\n",
    "\t# model.add(word_embeddings)\n",
    "\t# model.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))\n",
    "\t# input shape is a pair of (timesteps, features) <https://stackoverflow.com/a/44583784/1391325>\n",
    "\tinput_shape = training_x.shape[1:]\n",
    "\tprint(\"Input shape: {}\".format(input_shape), file=sys.stderr)\n",
    "\tunits = training_y.shape[1]\n",
    "\tprint(\"Units: {}\".format(units), file=sys.stderr)\n",
    "\tlstm = LSTM(input_shape=input_shape, units=units, dropout=0.1, recurrent_dropout=0.1)\n",
    "\t# lstm = LSTM(batch_input_shape = training_x.shape, stateful = True, units=len(training_y.shape))\n",
    "\tresult.add(lstm)\n",
    "\tresult.add(Dense(units, activation='softmax'))\n",
    "\tresult.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "\tprint(result.summary(), file=sys.stderr)\n",
    "\treturn result\n",
    "\n",
    "training_x, training_y = split_xy(training_batch)\n",
    "print(\"Training X shape: {}\".format(training_x.shape), file=sys.stderr)\n",
    "print(\"Training Y shape: {}\".format(training_y.shape), file=sys.stderr)\n",
    "model = create_model(training_x, training_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train LSTM\n",
    "\n",
    "epochs = 50\n",
    "print(\"Training model using {} epoch(s).\".format(epochs), file=sys.stderr)\n",
    "\n",
    "#import keras.utils\n",
    "#import math\n",
    "\n",
    "#class CIFAR10Sequence(keras.utils.Sequence):\n",
    "\n",
    "#    def __init__(self, x_set, y_set, batch_size):\n",
    "#        self.x, self.y = x_set, y_set\n",
    "#        self.batch_size = batch_size\n",
    "\n",
    "#    def __len__(self):\n",
    "#        return math.ceil(len(self.x) / self.batch_size)\n",
    "\n",
    "#    def __getitem__(self, idx):\n",
    "#        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "#        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "#        return np.array(batch_x), np.array(batch_y)\n",
    "    \n",
    "#training_data_generator = CIFAR10Sequence(training_x, training_y, 32)\n",
    "\n",
    "#test_x, test_y = split_xy(test_matrix)\n",
    "#print(\"Training X shape: {}\".format(test_x.shape), file=sys.stderr)\n",
    "#print(\"Training Y shape: {}\".format(test_y.shape), file=sys.stderr)\n",
    "\n",
    "#model.fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None)\n",
    "#training_history = model.fit(training_x, training_y, epochs=1, verbose=1, validation_data=(test_x, test_y))\n",
    "training_history = model.fit(training_x, training_y, epochs=epochs, verbose=0, validation_split=0.1)\n",
    "#for seq_idx in range(0, len(training_x)):\n",
    "#   seq_x = training_x[seq_idx]\n",
    "#    print(seq_x.shape)\n",
    "#    seq_y = training_y[seq_idx]\n",
    "#    print(seq_y.shape)\n",
    "#    print(training_matrix.shape)\n",
    "#    training_x = np.array([training_matrix[:, :-1]])\n",
    "#print(training_x.shape)\n",
    "#    assert len(training_x.shape) == 3\n",
    "#    training_y = np.array([training_matrix[:, -1]])\n",
    "#print(training_y.shape)\n",
    "#    assert len(training_y.shape) == 3\n",
    "#    training_history = model.fit(x=training_x, y=training_y, epochs=1, verbose=1, batch_size=1)\n",
    "#    training_history = model.fit(x=seq_x, y=seq_y, epochs=1, verbose=1)\n",
    "#fit_generator(self, generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0)\n",
    "#training_history = model.fit_generator(training_data_generator, epochs=1, verbose=1)\n",
    "\n",
    "# train LSTM <https://machinelearningmastery.com/memory-in-a-long-short-term-memory-network/>\n",
    "#epochs = 5\n",
    "#for i in range(epochs):\n",
    "#    for seq_idx in range(0, training_x.shape[0]):\n",
    "        #x = training_x[i]\n",
    "#        x = training_x[0:31]\n",
    " #       x_oldshape = x.shape\n",
    " #       x_newshape = (-1, x_oldshape[0], x_oldshape[1])\n",
    "        #x = np.reshape(x, x_newshape)\n",
    "        #print(\"x.shape = {}\".format(x.shape), file=sys.stderr)\n",
    "        #y = training_y[i]\n",
    " #       y = training_y[0:31]\n",
    " #       y_oldshape = y.shape\n",
    " #       y_newshape = (-1, y_oldshape[0])\n",
    "        #y = np.reshape(y, y_newshape)\n",
    "        #print(\"y.shape = {}\".format(y.shape), file=sys.stderr)\n",
    " #       model.fit(x, y, epochs=1, batch_size=1, verbose=1, shuffle=False)\n",
    " #       model.reset_states()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "\n",
    "# list all data in history\n",
    "#print(training_history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(training_history.history['acc'])\n",
    "plt.plot(training_history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(training_history.history['loss'])\n",
    "plt.plot(training_history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test LSTM\n",
    "\n",
    "import logging\n",
    "\n",
    "def onehot_encodings(features : np.ndarray, onehot_encoder) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    :param features: A 2D array of feature vectors, each of which starts with one-hot encoding features. Any other features follow those.\n",
    "    :param onehot_encoder: The instance used to encode the original values.\n",
    "    :return: A 2D numpy array consisting only of one-hot encoding features.\n",
    "    \"\"\"\n",
    "    feature_count = onehot_encoder.n_values_[0]\n",
    "    return features[:,:feature_count + 1]\n",
    "\n",
    "def onehot_encoded_word(onehot_encodings : np.ndarray, label_encoder) -> str:\n",
    "    # Check if there are any non-zero values\n",
    "    if onehot_encodings.any():\n",
    "        word_label = onehot_encodings.argmax()\n",
    "        result = label_encoder.inverse_transform([word_label])[0]\n",
    "    else:\n",
    "        result = \"(padding)\"\n",
    "    return result\n",
    "\n",
    "def word(features : np.ndarray, label_encoder, onehot_encoder) -> str:\n",
    "    feature_count = onehot_encoder.n_values_[0]\n",
    "    onehot = features[:feature_count + 1]\n",
    "    return onehot_encoded_word(onehot, label_encoder)\n",
    "\n",
    "def word_seq(x : np.ndarray, label_encoder, onehot_encoder) -> np.ndarray:\n",
    "    word_features = onehot_encodings(x, onehot_encoder)\n",
    "    return np.apply_along_axis(lambda arr : onehot_encoded_word(arr, label_encoder), 1, word_features)\n",
    "\n",
    "def create_padded_seq_matrix(group: pd.DataFrame, seq_matrix_factory):\n",
    "    seq_matrices = tuple(seq_matrix_factory(group))\n",
    "    print(\"Created a dataset composed of {} sequence(s), with a max sequence length of {}.\".format(len(seq_matrices), max(m.shape[0] for m in seq_matrices)), file=sys.stderr)\n",
    "    print(\"Padding data sequences.\", file=sys.stderr)\n",
    "    return keras.preprocessing.sequence.pad_sequences(seq_matrices, maxlen=None, padding='pre', truncating='pre', value=0.0)\n",
    "    \n",
    "\n",
    "def test_token_seq(token_seq_group : pd.DataFrame, seq_matrix_factory, model: Sequential, label_encoder, onehot_encoder): \n",
    "    entity_groups = token_seq_group.groupby(\"ENTITY\", as_index=False, sort=False)\n",
    "    entity_matrices = tuple(entity_groups.apply(lambda group : (group.name, seq_matrix_factory(group))))\n",
    "    #for _, m in entity_matrices:\n",
    "    #    y = m[:, -1]\n",
    "    #    print(y) \n",
    "    \n",
    "    logging.debug(\"Created matrices for %d entities, with a max sequence length of %d.\", len(entity_matrices), max(m.shape[0] for _, m in entity_matrices))\n",
    "    logging.debug(\"Padding entity test data sequences.\")\n",
    "    test_batch = keras.preprocessing.sequence.pad_sequences(tuple(m for _, m in entity_matrices), maxlen=None, padding='pre', value=0.0, dtype='int32')\n",
    "    print(test_batch[:, :, :-1])\n",
    "    logging.debug(\"Test batch shape: %s\", test_batch.shape)\n",
    "    test_x, test_y = split_xy(test_batch)\n",
    "    \n",
    "    \n",
    "    for group_name, m in entity_matrices:\n",
    "        m = np.expand_dims(m, axis=0)\n",
    "        print(m.shape)\n",
    "        x = m[:, :, :-1]\n",
    "        print(x.shape)\n",
    "        y = m[:, :, -1]\n",
    "        print(y.shape)\n",
    "        model.predict(x, verbose=0, batch_size=1)\n",
    "    \n",
    "    #logging.debug(\"Test X shape: %s\", test_x.shape)\n",
    "    #logging.debug(\"Test Y shape: %s\", test_y.shape)\n",
    "    # Classify all sets of datapoints for each entity as a single batch\n",
    "    #entity_predicted_scores = model.predict(test_x, verbose=0)\n",
    "    \n",
    "    max_score = 0\n",
    "    for idx in range(0, test_x.shape[0]):\n",
    "        entity_id = entity_matrices[idx][0]\n",
    "        # Get the scores for each entity\n",
    "        entity_x = test_x[idx]\n",
    "        logging.debug(\"Re-scoring token sequence for entity %s: %s\", entity_id, word_seq(entity_x, label_encoder, onehot_encoder))\n",
    "        entity_orig_scores = test_y[idx]\n",
    "        for tok_idx, features in enumerate(entity_x):\n",
    "            w = word(features, label_encoder, onehot_encoder)\n",
    "            s = entity_orig_scores[tok_idx]\n",
    "            print(\"{}; Orig score: {}\".format(w, s))\n",
    "        \n",
    "        max_score = max(max_score, entity_orig_scores.argmax())\n",
    "        #print(entity_orig_scores)\n",
    "        orig_score = entity_orig_scores.sum()\n",
    "        #predicted_scores = entity_predicted_scores[idx]\n",
    "        #print(predicted_scores)\n",
    "        #rescored = np.dot(entity_orig_scores, predicted_scores)\n",
    "        #logging.warning(\"Original score: %f; New score: %f\", orig_score, rescored)\n",
    "                        \n",
    "    print(\"Max score: {}\".format(max_score))\n",
    "            \n",
    "token_seq_groups = test_df.groupby(\n",
    "                (\"CROSS_VALIDATION_ITER\", \"DYAD\", \"ROUND\", \"UTT_START_TIME\", \"UTT_END_TIME\"),\n",
    "                as_index=False, sort=False)\n",
    "print(\"Will test {} token sequences.\".format(len(token_seq_groups)), file=sys.stderr)\n",
    "test_results = token_seq_groups.apply(lambda group: test_token_seq(group, seq_matrix_factory, model, label_encoder, onehot_encoder))\n",
    "\n",
    "                        \n",
    "#test_x, test_y = split_xy(test_matrix)\n",
    "#print(\"Training X shape: {}\".format(test_x.shape), file=sys.stderr)\n",
    "#print(\"Training Y shape: {}\".format(test_y.shape), file=sys.stderr)\n",
    "\n",
    "#seq_predicted_values = model.predict(test_x, verbose=0)\n",
    "#model.reset_states()\n",
    "#print(\"result.shape = {}\".format(seq_predicted_values.shape), file=sys.stderr)\n",
    "#for i, tested_seq in enumerate(test_x):\n",
    "#    word_features = onehot_encodings(tested_seq, onehot_encoder)\n",
    "    #print(\"word features: {}\".format(word_features))\n",
    "    #print(\"Tested sequence: {}\".format(tested_seq))\n",
    "    #print(\"Tested sequence shape: {}\".format(tested_seq.shape))\n",
    "    #seq_labels = word_features.argmax(axis=1)\n",
    "    #print(\"Inverse labels: {}\".format(inverse_labels))\n",
    "    #seq_words = label_encoder.inverse_transform(seq_labels)\n",
    "#    seq_words = np.apply_along_axis(lambda arr : onehot_encoded_word(arr, label_encoder), 1, word_features)\n",
    "    #print(\"Inverse word labels: {}\".format(seq_words))\n",
    "#    predicted_values = seq_predicted_values[i]\n",
    "#    assert tested_seq.shape[:-1] == predicted_values.shape\n",
    "    #assert tested_seq.shape[:-1] == actual_values.shape\n",
    "    #rint(\"Predicted values: {}\".format(predicted_values))\n",
    "#    actual_values = test_y[i]\n",
    "#    assert predicted_values.shape == actual_values.shape\n",
    "    \n",
    "#    differences = predicted_values - actual_values\n",
    "#    print(\"[Predicted_val - Actual_val]: {}\".format(differences))\n",
    "    #print(\"Actual values: {}\".format(actual_values))\n",
    "\t#print('X=%.1f y=%.1f, yhat=%.1f' % (seq1[i], seq1[i+1], result[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
