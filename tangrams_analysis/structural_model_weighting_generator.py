#!/usr/bin/env python3

"""
Rescores words-as-classifier scores using sequence prediction

Uses data generated by "structural_model_weighting_trainer.py".
"""

__author__ = "Todd Shore <errantlinguist+github@gmail.com>"
__copyright__ = "Copyright 2017 Todd Shore"
__license__ = "Apache License, Version 2.0"

import argparse
import random
import sys

import keras.preprocessing.sequence
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from keras.models import Sequential

from structural_model_weighting_trainer import DataGeneratorFactory, SequenceFeatureExtractor, TrainingFile, \
	find_target_ref_rows, group_seqs_by_len


def create_loss_plot(training_history):
	# https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/

	# list all data in history
	# print(training_history.history.keys())
	# summarize history for accuracy
	plt.plot(training_history.history['acc'])
	plt.plot(training_history.history['val_acc'])
	plt.title('model accuracy')
	plt.ylabel('accuracy')
	plt.xlabel('epoch')
	plt.legend(['train', 'test'], loc='upper left')
	plt.show()
	# summarize history for loss
	plt.plot(training_history.history['loss'])
	plt.plot(training_history.history['val_loss'])
	plt.title('model loss')
	plt.ylabel('loss')
	plt.xlabel('epoch')
	plt.legend(['train', 'test'], loc='upper left')
	plt.show()


def onehot_encodings(features: np.ndarray, onehot_encoder) -> np.ndarray:
	"""
	:param features: A 2D array of feature vectors, each of which starts with one-hot encoding features. Any other features follow those.
	:param onehot_encoder: The instance used to encode the original values.
	:return: A 2D numpy array consisting only of one-hot encoding features.
	"""
	feature_count = onehot_encoder.n_values_[0]
	return features[:, :feature_count + 1]


def onehot_encoded_word(onehot_features: np.ndarray, label_encoder) -> str:
	# Check if there are any non-zero values
	if onehot_features.any():
		word_label = onehot_features.argmax()
		result = label_encoder.inverse_transform([word_label])[0]
	else:
		result = "(padding)"
	return result


def word(features: np.ndarray, label_encoder, onehot_encoder) -> str:
	feature_count = onehot_encoder.n_values_[0]
	onehot = features[:feature_count + 1]
	return onehot_encoded_word(onehot, label_encoder)


def word_seq(x: np.ndarray, label_encoder, onehot_encoder) -> np.ndarray:
	word_features = onehot_encodings(x, onehot_encoder)
	return np.apply_along_axis(lambda arr: onehot_encoded_word(arr, label_encoder), 1, word_features)


def score_entity(entity_utts: pd.DataFrame, seq_feature_extractor, model: Sequential):
	sequence_groups = entity_utts.groupby(
		("UTT_START_TIME", "UTT_END_TIME"), sort=False)
	print("Generating data for {} entity token sequence(s).".format(len(sequence_groups)), file=sys.stderr)
	# for time, tok_seq in sequence_groups:
	#    print(time)
	#    print(tok_seq)

	seq_xy = sequence_groups.apply(seq_feature_extractor)
	len_dict = group_seqs_by_len(seq_xy)
	print("Created {} batches, one for each unique sequence length.".format(len(len_dict)), file=sys.stderr)
	for seq_len, seqs in sorted(len_dict.items(), key=lambda item: item[0]):
		print(seqs)
	seq_batches_by_len = tuple(len_dict.values())
	for xy in seq_batches_by_len:
		print(type(xy))


def test_round(round_group: pd.DataFrame, seq_feature_extractor, model: Sequential):
	entity_utts = round_group.groupby(("ENTITY", "IS_TARGET"), as_index=False, sort=False)
	entity_utts.apply(lambda group: score_entity(group, seq_feature_extractor, model))


def __create_argparser() -> argparse.ArgumentParser:
	result = argparse.ArgumentParser(
		description="Rescores words-as-classifier scores using sequence prediction.")
	result.add_argument("indir", metavar="DIR", help="The directory with the model data to use for classification.")
	return result


def __main(args):
	indir = args.indir
	print("Will read data from \"{}\".".format(indir),
		  file=sys.stderr)

	random_seed = TrainingFile.RANDOM_SEED.value.read(indir)
	print("Setting random seed to {}.".format(random_seed), file=sys.stderr)
	# https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/
	# fix random seed for reproducibility
	random.seed(random_seed)
	np.random.seed(random_seed)

	label_encoder = TrainingFile.VOCAB_LABELS.value.read(indir)
	onehot_encoder = TrainingFile.ONEHOT_ENCODINGS.value.read(indir)
	# assert onehot_encoder.n_values_ == len(vocab_words)
	# vocab_onehot_encoded = onehot_encoder.fit_transform(vocab_labels)
	# print(vocab_onehot_encoded)
	# invert first example
	# inverted = label_encoder.inverse_transform([np.argmax(vocab_onehot_encoded[0, :])])
	# print(inverted)

	# training_df = TrainingFile.TRAINING_DATA.value.read(indir)
	test_df = TrainingFile.TEST_DATA.value.read(indir)

	seq_feature_extractor = SequenceFeatureExtractor(onehot_encoder)
	data_generator_factory = DataGeneratorFactory(seq_feature_extractor)
	# print("Generating training data token sequences.", file=sys.stderr)
	# training_data_generator = data_generator_factory(training_df)
	print("Generating validation data token sequences.", file=sys.stderr)
	validation_data_generator = data_generator_factory(find_target_ref_rows(test_df))

	# https://stackoverflow.com/a/43472000/1391325
	with keras.backend.get_session():
		model = TrainingFile.MODEL.value.read(indir)
		# create_loss_plot(training_history)
		round_utts = test_df.groupby(
			("CROSS_VALIDATION_ITER", "DYAD", "ROUND"),
			as_index=False, sort=False)
		print("Will test {} rounds.".format(len(round_utts)), file=sys.stderr)
		test_results = round_utts.apply(
			lambda group: test_round(group, seq_feature_extractor, model))


if __name__ == "__main__":
	__main(__create_argparser().parse_args())
